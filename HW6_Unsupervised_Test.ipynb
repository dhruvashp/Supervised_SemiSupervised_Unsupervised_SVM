{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we perform \n",
    "A, B, C of the unsupervised portion for a single run (single, random Monte-Carlo iteration) before performing it for M=30 iterations\n",
    "\n",
    "\n",
    "NOTE : It hasn't been asked to normalize the data here, so we'll go with the raw data, the raw features as given originally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        X1     X2      X3      X4       X5       X6       X7       X8      X9  \\\n",
      "0    17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710  0.2419   \n",
      "1    20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017  0.1812   \n",
      "2    19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790  0.2069   \n",
      "3    11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520  0.2597   \n",
      "4    20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430  0.1809   \n",
      "..     ...    ...     ...     ...      ...      ...      ...      ...     ...   \n",
      "564  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390  0.13890  0.1726   \n",
      "565  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400  0.09791  0.1752   \n",
      "566  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251  0.05302  0.1590   \n",
      "567  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140  0.15200  0.2397   \n",
      "568   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000  0.1587   \n",
      "\n",
      "         X10  ...    X22     X23     X24      X25      X26     X27     X28  \\\n",
      "0    0.07871  ...  17.33  184.60  2019.0  0.16220  0.66560  0.7119  0.2654   \n",
      "1    0.05667  ...  23.41  158.80  1956.0  0.12380  0.18660  0.2416  0.1860   \n",
      "2    0.05999  ...  25.53  152.50  1709.0  0.14440  0.42450  0.4504  0.2430   \n",
      "3    0.09744  ...  26.50   98.87   567.7  0.20980  0.86630  0.6869  0.2575   \n",
      "4    0.05883  ...  16.67  152.20  1575.0  0.13740  0.20500  0.4000  0.1625   \n",
      "..       ...  ...    ...     ...     ...      ...      ...     ...     ...   \n",
      "564  0.05623  ...  26.40  166.10  2027.0  0.14100  0.21130  0.4107  0.2216   \n",
      "565  0.05533  ...  38.25  155.00  1731.0  0.11660  0.19220  0.3215  0.1628   \n",
      "566  0.05648  ...  34.12  126.70  1124.0  0.11390  0.30940  0.3403  0.1418   \n",
      "567  0.07016  ...  39.42  184.60  1821.0  0.16500  0.86810  0.9387  0.2650   \n",
      "568  0.05884  ...  30.37   59.16   268.6  0.08996  0.06444  0.0000  0.0000   \n",
      "\n",
      "        X29      X30  y  \n",
      "0    0.4601  0.11890  1  \n",
      "1    0.2750  0.08902  1  \n",
      "2    0.3613  0.08758  1  \n",
      "3    0.6638  0.17300  1  \n",
      "4    0.2364  0.07678  1  \n",
      "..      ...      ... ..  \n",
      "564  0.2060  0.07115  1  \n",
      "565  0.2572  0.06637  1  \n",
      "566  0.2218  0.07820  1  \n",
      "567  0.4087  0.12400  1  \n",
      "568  0.2871  0.07039  0  \n",
      "\n",
      "[569 rows x 31 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DHRUV\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('wdbc.csv', header = None)\n",
    "\n",
    "\n",
    "output = df.iloc[:,0]\n",
    "\n",
    "\n",
    "for i in np.arange(0,df.shape[0]):\n",
    "    if output.iloc[i] == 'B':\n",
    "        output.iloc[i] = 0\n",
    "    else:\n",
    "        output.iloc[i] = 1\n",
    "\n",
    "\n",
    "\n",
    "features = df.iloc[:,1:]\n",
    "\n",
    "whole = pd.concat([features,output],axis=1)   # Raw features used, normalization not asked for in the question\n",
    "\n",
    "whole.columns= ['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13','X14','X15','X16','X17','X18','X19','X20','X21','X22','X23','X24','X25','X26','X27','X28','X29','X30','y']\n",
    "\n",
    "print(whole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the features have been kept as is and haven't been normalized. This wasn't asked in the question and thus has been skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        X1     X2      X3     X4       X5       X6       X7       X8      X9  \\\n",
      "0    12.89  15.70   84.08  516.6  0.07818  0.09580  0.11150  0.03390  0.1432   \n",
      "1    15.19  13.21   97.65  711.8  0.07963  0.06934  0.03393  0.02657  0.1721   \n",
      "2    12.21  18.02   78.31  458.4  0.09231  0.07175  0.04392  0.02027  0.1695   \n",
      "3    13.40  16.95   85.48  552.4  0.07937  0.05696  0.02181  0.01473  0.1650   \n",
      "4    16.50  18.29  106.60  838.1  0.09686  0.08468  0.05862  0.04835  0.1495   \n",
      "..     ...    ...     ...    ...      ...      ...      ...      ...     ...   \n",
      "352  11.89  18.35   77.32  432.2  0.09363  0.11540  0.06636  0.03142  0.1967   \n",
      "353   8.95  15.76   58.74  245.2  0.09462  0.12430  0.09263  0.02308  0.1305   \n",
      "354  12.00  15.65   76.95  443.3  0.09723  0.07165  0.04151  0.01863  0.2079   \n",
      "355  11.41  10.82   73.34  403.3  0.09373  0.06685  0.03512  0.02623  0.1667   \n",
      "356  14.62  24.02   94.57  662.7  0.08974  0.08606  0.03102  0.02957  0.1685   \n",
      "\n",
      "         X10  ...    X22     X23     X24      X25     X26      X27      X28  \\\n",
      "0    0.05935  ...  19.69   92.12   595.6  0.09926  0.2317  0.33440  0.10170   \n",
      "1    0.05544  ...  15.73  104.50   819.1  0.11260  0.1737  0.13620  0.08178   \n",
      "2    0.05916  ...  24.04   93.85   624.6  0.13680  0.2170  0.24130  0.08829   \n",
      "3    0.05701  ...  21.70   93.76   663.5  0.12130  0.1676  0.13640  0.06987   \n",
      "4    0.05593  ...  25.45  117.20  1009.0  0.13380  0.1679  0.16630  0.09123   \n",
      "..       ...  ...    ...     ...     ...      ...     ...      ...      ...   \n",
      "352  0.06314  ...  27.10   86.20   531.2  0.14050  0.3046  0.28060  0.11380   \n",
      "353  0.07163  ...  17.07   63.34   270.0  0.11790  0.1879  0.15440  0.03846   \n",
      "354  0.05968  ...  24.90   87.78   567.9  0.13770  0.2003  0.22670  0.07632   \n",
      "355  0.06113  ...  15.97   83.74   510.5  0.15480  0.2390  0.21020  0.08958   \n",
      "356  0.05866  ...  29.11  102.90   803.7  0.11150  0.1766  0.09189  0.06946   \n",
      "\n",
      "        X29      X30  y  \n",
      "0    0.1999  0.07127  0  \n",
      "1    0.2487  0.06766  0  \n",
      "2    0.3218  0.07470  0  \n",
      "3    0.2741  0.07582  0  \n",
      "4    0.2394  0.06469  0  \n",
      "..      ...      ... ..  \n",
      "352  0.3397  0.08365  0  \n",
      "353  0.1652  0.07722  0  \n",
      "354  0.3379  0.07924  0  \n",
      "355  0.3016  0.08523  0  \n",
      "356  0.2522  0.07246  0  \n",
      "\n",
      "[357 rows x 31 columns]\n",
      "        X1     X2      X3      X4       X5       X6       X7       X8      X9  \\\n",
      "0    21.10  20.52  138.10  1384.0  0.09684  0.11750  0.15720  0.11550  0.1554   \n",
      "1    15.37  22.76  100.20   728.2  0.09200  0.10360  0.11220  0.07483  0.1717   \n",
      "2    19.59  25.00  127.70  1191.0  0.10320  0.09871  0.16550  0.09063  0.1663   \n",
      "3    14.78  23.94   97.40   668.3  0.11720  0.14790  0.12670  0.09029  0.1953   \n",
      "4    18.05  16.15  120.20  1006.0  0.10650  0.21460  0.16840  0.10800  0.2152   \n",
      "..     ...    ...     ...     ...      ...      ...      ...      ...     ...   \n",
      "207  11.80  16.58   78.99   432.0  0.10910  0.17000  0.16590  0.07415  0.2678   \n",
      "208  11.08  18.83   73.30   361.6  0.12160  0.21540  0.16890  0.06367  0.2196   \n",
      "209  16.11  18.05  105.10   813.0  0.09721  0.11370  0.09447  0.05943  0.1861   \n",
      "210  14.95  17.57   96.85   678.1  0.11670  0.13050  0.15390  0.08624  0.1957   \n",
      "211  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710  0.2419   \n",
      "\n",
      "         X10  ...    X22     X23     X24     X25     X26     X27     X28  \\\n",
      "0    0.05661  ...  32.07  168.20  2022.0  0.1368  0.3101  0.4399  0.2280   \n",
      "1    0.06097  ...  25.84  107.50   830.9  0.1257  0.1997  0.2846  0.1476   \n",
      "2    0.05391  ...  30.96  139.80  1421.0  0.1528  0.1845  0.3977  0.1466   \n",
      "3    0.06654  ...  33.39  114.60   925.1  0.1648  0.3416  0.3024  0.1614   \n",
      "4    0.06673  ...  18.91  150.10  1610.0  0.1478  0.5634  0.3786  0.2102   \n",
      "..       ...  ...    ...     ...     ...     ...     ...     ...     ...   \n",
      "207  0.07371  ...  26.38   91.93   591.7  0.1385  0.4092  0.4504  0.1865   \n",
      "208  0.07950  ...  32.82   91.76   508.1  0.2184  0.9379  0.8402  0.2524   \n",
      "209  0.06248  ...  25.27  129.00  1233.0  0.1314  0.2236  0.2802  0.1216   \n",
      "210  0.06216  ...  21.43  121.40   971.4  0.1411  0.2164  0.3355  0.1667   \n",
      "211  0.07871  ...  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
      "\n",
      "        X29      X30  y  \n",
      "0    0.2268  0.07425  1  \n",
      "1    0.2556  0.06828  1  \n",
      "2    0.2293  0.06091  1  \n",
      "3    0.3321  0.08911  1  \n",
      "4    0.3751  0.11080  1  \n",
      "..      ...      ... ..  \n",
      "207  0.5774  0.10300  1  \n",
      "208  0.4154  0.14030  1  \n",
      "209  0.2792  0.08158  1  \n",
      "210  0.3414  0.07147  1  \n",
      "211  0.4601  0.11890  1  \n",
      "\n",
      "[212 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "whole.sort_values(by=['y'],inplace=True)\n",
    "whole.reset_index(drop=True,inplace=True)\n",
    "X_y_0 = whole.iloc[0:357,:]\n",
    "X_y_1 = whole.iloc[357:569,:]\n",
    "X_y_1.reset_index(drop=True,inplace=True)\n",
    "print(X_y_0)             \n",
    "print(X_y_1)\n",
    "# Only 0 and 1 portions of the entire dataset, to obtain the required test proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         X1     X2      X3      X4       X5       X6        X7        X8  \\\n",
      "0     9.465  21.01   60.11   269.4  0.10440  0.07773  0.021720  0.015040   \n",
      "1    13.210  25.25   84.10   537.9  0.08791  0.05205  0.027720  0.020680   \n",
      "2    12.680  23.84   82.69   499.0  0.11220  0.12620  0.112800  0.068730   \n",
      "3    17.570  15.05  115.00   955.1  0.09847  0.11570  0.098750  0.079530   \n",
      "4    14.220  23.12   94.37   609.9  0.10750  0.24130  0.198100  0.066180   \n",
      "..      ...    ...     ...     ...      ...      ...       ...       ...   \n",
      "449  11.220  33.81   70.79   386.8  0.07780  0.03574  0.004967  0.006434   \n",
      "450  17.050  19.08  113.40   895.0  0.11410  0.15720  0.191000  0.109000   \n",
      "451   9.738  11.97   61.24   288.5  0.09250  0.04102  0.000000  0.000000   \n",
      "452  11.990  24.89   77.61   441.3  0.10300  0.09218  0.054410  0.042740   \n",
      "453  20.920  25.09  143.00  1347.0  0.10990  0.22360  0.317400  0.147400   \n",
      "\n",
      "         X9      X10  ...    X21    X22     X23     X24      X25      X26  \\\n",
      "0    0.1717  0.06899  ...  10.41  31.56   67.03   330.7  0.15480  0.16640   \n",
      "1    0.1619  0.05584  ...  14.35  34.23   91.29   632.9  0.12890  0.10630   \n",
      "2    0.1905  0.06590  ...  17.09  33.47  111.80   888.3  0.18510  0.40610   \n",
      "3    0.1739  0.06149  ...  20.01  19.52  134.90  1227.0  0.12550  0.28120   \n",
      "4    0.2384  0.07542  ...  15.74  37.18  106.40   762.4  0.15330  0.93270   \n",
      "..      ...      ...  ...    ...    ...     ...     ...      ...      ...   \n",
      "449  0.1845  0.05828  ...  12.36  41.78   78.44   470.9  0.09994  0.06885   \n",
      "450  0.2131  0.06325  ...  19.59  24.89  133.50  1189.0  0.17030  0.39340   \n",
      "451  0.1903  0.06422  ...  10.62  14.10   66.53   342.9  0.12340  0.07204   \n",
      "452  0.1820  0.06850  ...  12.98  30.36   84.48   513.9  0.13110  0.18220   \n",
      "453  0.2149  0.06879  ...  24.29  29.41  179.10  1819.0  0.14070  0.41860   \n",
      "\n",
      "         X27      X28     X29      X30  \n",
      "0    0.09412  0.06517  0.2878  0.09211  \n",
      "1    0.13900  0.06005  0.2444  0.06788  \n",
      "2    0.40240  0.17160  0.3383  0.10310  \n",
      "3    0.24890  0.14560  0.2756  0.07919  \n",
      "4    0.84880  0.17720  0.5166  0.14460  \n",
      "..       ...      ...     ...      ...  \n",
      "449  0.02318  0.03002  0.2911  0.07307  \n",
      "450  0.50180  0.25430  0.3109  0.09061  \n",
      "451  0.00000  0.00000  0.3105  0.08151  \n",
      "452  0.16090  0.12020  0.2599  0.08251  \n",
      "453  0.65990  0.25420  0.2929  0.09873  \n",
      "\n",
      "[454 rows x 30 columns]\n",
      "0      0\n",
      "1      0\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "449    0\n",
      "450    1\n",
      "451    0\n",
      "452    0\n",
      "453    1\n",
      "Name: y, Length: 454, dtype: int32\n",
      "         X1     X2      X3      X4       X5       X6        X7        X8  \\\n",
      "0    13.820  24.49   92.33   595.9  0.11620  0.16810  0.135700  0.067590   \n",
      "1    12.460  12.83   78.83   477.3  0.07372  0.04043  0.007173  0.011490   \n",
      "2    11.040  16.83   70.92   373.2  0.10770  0.07804  0.030460  0.024800   \n",
      "3     9.787  19.94   62.11   294.5  0.10240  0.05301  0.006829  0.007937   \n",
      "4     9.405  21.70   59.60   271.2  0.10440  0.06159  0.020470  0.012570   \n",
      "..      ...    ...     ...     ...      ...      ...       ...       ...   \n",
      "110  11.840  18.94   75.51   428.0  0.08871  0.06900  0.026690  0.013930   \n",
      "111  20.440  21.78  133.80  1293.0  0.09150  0.11310  0.097990  0.077850   \n",
      "112  10.440  15.46   66.62   329.6  0.10530  0.07722  0.006643  0.012160   \n",
      "113  13.590  21.84   87.16   561.0  0.07956  0.08259  0.040720  0.021420   \n",
      "114  13.640  16.34   87.21   571.8  0.07685  0.06059  0.018570  0.017230   \n",
      "\n",
      "         X9      X10  ...    X21    X22     X23     X24      X25      X26  \\\n",
      "0    0.2275  0.07237  ...  16.01  32.94  106.00   788.0  0.17940  0.39660   \n",
      "1    0.1613  0.06013  ...  13.19  16.36   83.24   534.0  0.09439  0.06477   \n",
      "2    0.1714  0.06340  ...  12.41  26.44   79.93   471.4  0.13690  0.14820   \n",
      "3    0.1350  0.06890  ...  10.92  26.29   68.81   366.1  0.13160  0.09473   \n",
      "4    0.2025  0.06601  ...  10.85  31.24   68.73   359.4  0.15260  0.11930   \n",
      "..      ...      ...  ...    ...    ...     ...     ...      ...      ...   \n",
      "110  0.1533  0.06057  ...  13.30  24.99   85.22   546.3  0.12800  0.18800   \n",
      "111  0.1618  0.05557  ...  24.31  26.37  161.20  1780.0  0.13270  0.23760   \n",
      "112  0.1788  0.06450  ...  11.52  19.80   73.47   395.4  0.13410  0.11530   \n",
      "113  0.1635  0.05859  ...  14.80  30.04   97.66   661.5  0.10050  0.17300   \n",
      "114  0.1353  0.05953  ...  14.67  23.19   96.08   656.7  0.10890  0.15820   \n",
      "\n",
      "         X27      X28     X29      X30  \n",
      "0    0.33810  0.15210  0.3651  0.11830  \n",
      "1    0.01674  0.02680  0.2280  0.07028  \n",
      "2    0.10670  0.07431  0.2998  0.07881  \n",
      "3    0.02049  0.02381  0.1934  0.08988  \n",
      "4    0.06141  0.03770  0.2872  0.08304  \n",
      "..       ...      ...     ...      ...  \n",
      "110  0.14710  0.06913  0.2535  0.07993  \n",
      "111  0.27020  0.17650  0.2609  0.06735  \n",
      "112  0.02639  0.04464  0.2615  0.08269  \n",
      "113  0.14530  0.06189  0.2446  0.07024  \n",
      "114  0.10500  0.08586  0.2346  0.08025  \n",
      "\n",
      "[115 rows x 30 columns]\n",
      "0      1\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "110    0\n",
      "111    1\n",
      "112    0\n",
      "113    0\n",
      "114    0\n",
      "Name: y, Length: 115, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# Again, note normalization hasn't been performed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_y_train_0,X_y_test_0 = train_test_split(X_y_0,test_size = 0.2,shuffle=True)\n",
    "X_y_train_1,X_y_test_1 = train_test_split(X_y_1,test_size = 0.2,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "X_y_train = pd.concat([X_y_train_0,X_y_train_1],axis=0)\n",
    "X_y_test = pd.concat([X_y_test_0,X_y_test_1],axis=0)\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_y_train = shuffle(X_y_train)\n",
    "X_y_test = shuffle(X_y_test)\n",
    "\n",
    "\n",
    "\n",
    "X_y_train.reset_index(drop=True,inplace=True)\n",
    "X_y_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "X_train = X_y_train.drop(columns=['y'])\n",
    "X_test = X_y_test.drop(columns=['y'])\n",
    "y_train = X_y_train['y'].astype(int)\n",
    "y_test =  X_y_test['y'].astype(int)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)\n",
    "\n",
    "\n",
    "# the above code generates the train and test sets, raw, with the required class proportions, as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1\n",
      " 1 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1\n",
      " 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0\n",
      " 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0\n",
      " 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1\n",
      " 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0\n",
      " 1 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1\n",
      " 1 1 1 1 0 1 0 1 1 0]\n",
      "[[1.95524272e+01 2.17500000e+01 1.29460194e+02 1.20618252e+03\n",
      "  1.01922524e-01 1.50721748e-01 1.80237864e-01 1.02577379e-01\n",
      "  1.90916505e-01 6.08117476e-02 7.35649515e-01 1.19354078e+00\n",
      "  5.16933981e+00 9.52884466e+01 6.60421359e-03 3.20609223e-02\n",
      "  4.17117476e-02 1.52902718e-02 2.00762136e-02 3.98317476e-03\n",
      "  2.39249515e+01 2.89690291e+01 1.60020388e+02 1.77772816e+03\n",
      "  1.41975922e-01 3.67938447e-01 4.58076699e-01 1.95795243e-01\n",
      "  3.13346602e-01 8.75563107e-02]\n",
      " [1.26033191e+01 1.86830769e+01 8.14434473e+01 5.00059259e+02\n",
      "  9.51222222e-02 9.14981481e-02 6.25428966e-02 3.36345470e-02\n",
      "  1.79004558e-01 6.34256410e-02 3.12915954e-01 1.20451994e+00\n",
      "  2.21109202e+00 2.46946895e+01 7.09937892e-03 2.34884929e-02\n",
      "  2.86166484e-02 1.06598946e-02 2.08581425e-02 3.75445385e-03\n",
      "  1.41354131e+01 2.48065242e+01 9.25677778e+01 6.28340741e+02\n",
      "  1.29631311e-01 2.23220313e-01 2.18985541e-01 9.08226524e-02\n",
      "  2.85869231e-01 8.30214815e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Performing K-Means on the entire train, with k = 2\n",
    "# We perform K-Means multiple times, to ensure convergence to global optimum, each time with random initial assignment\n",
    "\n",
    "# In actuality, and ideally, k-means should be performed randomly multiple times, each time with an initial random \n",
    "# cluster assignment. This assignment should be stored, for each random iteration, and the objective function (function to min.)\n",
    "# for k-means also stored for each random iteration\n",
    "\n",
    "# Then the initial random assignment that gives the corresponding minimum objective function, of all the iterations, should\n",
    "# then be picked. This initial random cluster assignment is the assignment that actually leads to a global convergence (global min.)\n",
    "\n",
    "# Here, again, this won't be done. Reason mostly stems from making the problem simple\n",
    "\n",
    "# We will perform K-means multiple times here, each time with a random initial cluster assignment, in scikit, and assume convergence\n",
    "# by the time the algorithm picks the final random cluster assignment.\n",
    "\n",
    "# Again, while not exactly true, storing the initial cluster assignments for each iteration, calculating objective function\n",
    "# for each iteration, then selecting the assignment that leads to the most minimal objective function and re-clustering over\n",
    "# that assignment is relatively complicated (computationally, but also somewhat algorithmically).\n",
    "\n",
    "# Thus, from multiple random k-means trials, we assume eventual convergence\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for i in np.arange(0,10):        # refitting k-means 10 times \n",
    "                                 # only 10, to save computational intensity, can be increased at the price/cost of run times\n",
    "    kmeans = KMeans(n_clusters=2,init='random',n_init=10).fit(X_train)   # fitting clusters on entire train, with random initial cluster assignments\n",
    "\n",
    "    \n",
    "# NOTE : Both KMeans and SpectralCLustering have n_init = 10. As such the external loop is not required, but since it was used for \n",
    "# KMeans we will also use it for SpectralClustering. In all, again, the final fit will be the last one, and the total randomized\n",
    "# trials will be 10*10, increasing our chances of picking optimal clusters (globally optimal)\n",
    "\n",
    "    \n",
    "# the k-means performed at the end i.e. for i=9 or for the 10th time is chosen for subsequent portions\n",
    "\n",
    "# we assume by the 10th time or by i=9, the kmeans fit over X_train, selects an appropriately selected random initial cluster\n",
    "# assignment that leads to global convergence\n",
    "\n",
    "# we could 'poll' the results of all kmeans, or 'poll' over the obtained clusters, to obtain the polled cluster assignment\n",
    "# however this isn't done. One reason being this increases complexity. Another being this, then has to be done every\n",
    "# single time, whenever clusters are found over any test set. Also it required storage of the random initial assignments\n",
    "# for consistency\n",
    "\n",
    "# thus polling isn't done, and we assume that there is an 'iterative convergence'\n",
    "# additionally storing objective and initial clusters also isn't done for simplicity\n",
    "# also, as Monte-Carlo runs, we assume, that the 'averaged' parameters point to the globally converged set of clusters\n",
    "\n",
    "# thus, in conclusion, last fitted kmeans = KMeans(parameters).fit(X_train) is our fit over which subsequent portions are thus\n",
    "# done\n",
    "\n",
    "\n",
    "y_train_pred_cluster = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "print(y_train_pred_cluster)    \n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters predicted and the cluster centers are obtained as above\n",
    "\n",
    "PART A\n",
    "There are a few ways to guarantee kmeans convergence. By performing kmeans multiple times, with random initialized cluster assignments, we can guarantee that at least one of those few kmeans iterations will converge to the global optimum\n",
    "\n",
    "As such, this can be checked by measuring the kmeans objective functions for each iteration, storing it, storing the corresponding cluster assignment (which was done at random). The assignment that minimizes the objective over all the local optimals, thus give us the cluster assignment that converges to the global optimal.\n",
    "\n",
    "Obviously while this is the most accurate way to do kmeans, it takes up a lot of computation, run time and memory. If various scores for kmeans are needed, we assume by performing kmeans multiple times (Monte-Carlo) and storing the scores each time, the overall average score, corresponds to the one obtained via the globally optimum trained kmeans.\n",
    "\n",
    "Thus the external Monte-Carlo iterations, more or less, guarantee globally optimal converged scores\n",
    "\n",
    "NOTE : \n",
    "\n",
    "For each Monte iteration, if the optimal k-means is needed that also increases computation. Within each Monte-Carlo iteration, off of the multiple ways to try for kmeans convergence, a few are thus:\n",
    "- We can, for each Monte iteration, train kmeans multiple times, using random cluster assignments each time. Then 'poll' the       obtained kmeans labels over all the obtained labels. Something similar may be done for the 'polled' cluster centers\n",
    "  However, as explained in comments, this then needs to be performed for all fits and predictions with multiple complications of   storage and polling functions to define the 'kmeans' final fit function\n",
    "  This, is complicated, time consuming, and to a certain extent not needed due to the external Monte-Carlo iterations\n",
    "  \n",
    "- We can, for each Monte iteration, store objective and initial random cluster assignment for all within Monte kmeans\n",
    "  iterations. Then we pick the initial assignment that minimizes this 'minimized objective' over all local optimals.\n",
    "  This, again, is complicated, computationally intensive, requires a lot of storage.\n",
    "  Note, that this is the absolutely correct method. Not needed even for Monte-Carlo (as we do perform Monte iterations             externally)\n",
    "  \n",
    "- We can, for each Monte iteration, train kmeans multiple times, each time with a random initial cluster assignment. We assume     then, that the final fit corresponds to the globally convergent point. Again, while not true, this multiple iteration           approach with the selection of the final/last fit, does more or less guarantee an eventual convergence to global optimal         point. Multiple iterations, with the selection of last one, definitely increases our chances of selecting the globally           optimal fit.\n",
    "  Computationally easy, uncomplicated.\n",
    "  \n",
    "The second approach is the best and correct. The first approach is a good approximate to the second one. The third one is the easiest, most practical and least complicated.\n",
    "\n",
    "Noting, that, Monte-Carlo, is as is performed, we may use the third 'within-each-Monte-iteration' approach, since Monte does ensure that the scores, the averaged ones, do reflect the nature of globally converged kmeans fit over our dataset.\n",
    "\n",
    "Thus the third-within-monte-iteration approach has been used in the coding portion above this markdown due to its practicality and its convenience and due to the above mentioned facts.\n",
    "\n",
    "Results for B. and C. in subsequent coding portions, Monte performed in a seperate file, this is for one such Monte iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[304  21 111  15  66 358 184 330 391 353 369 428 328  48  86 107 162 235\n",
      " 453 155 432 320 383 183  83  82 366 141 316 337]\n",
      "[ 74 292 345  18 324 209 306 151  31 421 180  93 194 318 443 397 247 425\n",
      " 171  58 444 165 198 402 323 252 322   1 390  78]\n"
     ]
    }
   ],
   "source": [
    "# Calculating sample distance for each sample in X_train from their corresponding cluster center\n",
    "\n",
    "X_train_dist_0 = kmeans.transform(X_train)[:,0]\n",
    "X_train_dist_1 = kmeans.transform(X_train)[:,1]\n",
    "\n",
    "# distance arrays from cluster center 0 and 1 for all samples\n",
    "\n",
    "all_indx = np.arange(0,X_train.shape[0])           # all indices of X_train, stored in an array\n",
    "\n",
    "list_cluster_0 = []         # indices of samples classified to cluster 0\n",
    "list_cluster_1 = []         # indices of samples classified to cluster 1\n",
    "\n",
    "for i in all_indx:\n",
    "    if y_train_pred_cluster[i] == 0:\n",
    "        list_cluster_0.append(i)\n",
    "    else:\n",
    "        list_cluster_1.append(i)\n",
    "        \n",
    "# at the end of this for loop, list_cluster_0 and list_cluster_1 contain indices of all samples (internal indices) clustered\n",
    "# to cluster 0 and cluster 1 respectively\n",
    "\n",
    "dist_0_0 = X_train_dist_0[list_cluster_0]   # distance of samples classified to cluster 0 from cluster center 0\n",
    "dist_1_1 = X_train_dist_1[list_cluster_1]   # distance of samples classified to cluster 1 from cluster center 1\n",
    "\n",
    "# list_cluster_0 and list_cluster_1 has indices of samples in X_train (internal) classified to cluster 0 and cluster 1\n",
    "# the indices of samples in cluster 0 (list_cluster_0) slices the array that contains distances of all samples from cluster center\n",
    "# 0. This is stored in dist_0_0. Same for dist_1_1\n",
    "\n",
    "# thus dist_0_0 and dist_1_1 contains distances of samples from cluster center 0 and cluster center 1, but only those samples\n",
    "# that correspond to sample index in list_cluster_0 and list_cluster_1, which again contain only samples classified to \n",
    "# cluster 0 and cluster 1\n",
    "\n",
    "# so, dist_0_0 ---- list_cluster_0 --------- sample index in X_train in cluster 0\n",
    "# so, dist_1_1 ---- list_cluster_1 --------- sample index in X_train in cluster 1\n",
    "\n",
    "df_0 = pd.DataFrame(np.transpose(np.array([np.array(list_cluster_0),dist_0_0])),columns=['index','distance'])\n",
    "df_1 = pd.DataFrame(np.transpose(np.array([np.array(list_cluster_1),dist_1_1])),columns=['index','distance'])\n",
    "\n",
    "#  df for 0 and 1 (0/1) :\n",
    "\n",
    "#                 col                              col \n",
    "#  row    index (0/1 sample)         distance of corresponding sample from 0/1 cluster\n",
    "#  row    index (0/1 sample)         distance of corresponding sample from 0/1 cluster\n",
    "#  row    index (0/1 sample)         distance of corresponding sample from 0/1 cluster\n",
    "#  row    index (0/1 sample)         distance of corresponding sample from 0/1 cluster\n",
    "#   .           .                                   .\n",
    "#   .           .                                   . \n",
    "#   .           .                                   .\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sorting to pick 30 closest for each\n",
    "\n",
    "# we will assume that at least 30 points have been classified to both clusters (cluster 0 and cluster 1 have 30 points at least)\n",
    "\n",
    "# we could, code, extra logic to ensure that when this doesn't happen (say cluster 0 has less than 30 samples or cluster 1)\n",
    "# polling is done consequently\n",
    "\n",
    "# however looking at the dataset this issue shouldn't arise, especially given that 30 is a pretty low number\n",
    "\n",
    "# extra code for the 'weird case' would, as is, only take 1-2 lines, not more, for simplicity this 'what if?' case has \n",
    "# been ignored as we assume for this dataset this shouldn't happen in line with how it is distributed\n",
    "\n",
    "# dimensions are pretty large, so we should be able to obtain 'consequential clusters' such that this doesn't happen\n",
    "\n",
    "\n",
    "df_0_srt_slc = df_0.sort_values(by=['distance'],ascending=True).iloc[0:30,:]\n",
    "df_1_srt_slc = df_1.sort_values(by=['distance'],ascending=True).iloc[0:30,:]\n",
    "\n",
    "sel_indx_0_cls = df_0_srt_slc.iloc[:,0].to_numpy().flatten().astype(int)\n",
    "sel_indx_1_cls = df_1_srt_slc.iloc[:,0].to_numpy().flatten().astype(int)\n",
    "\n",
    "print(sel_indx_0_cls)\n",
    "print(sel_indx_1_cls)\n",
    "\n",
    "\n",
    "# sel_indx_0/1_cls contains the index of the samples in X_train classified to cluster 0/1 closest to cluster center 0/1\n",
    "# (total 30 such indices obviously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304    1\n",
      "21     1\n",
      "111    1\n",
      "15     1\n",
      "66     1\n",
      "358    1\n",
      "184    1\n",
      "330    1\n",
      "391    1\n",
      "353    1\n",
      "369    1\n",
      "428    1\n",
      "328    1\n",
      "48     1\n",
      "86     1\n",
      "107    1\n",
      "162    1\n",
      "235    1\n",
      "453    1\n",
      "155    1\n",
      "432    1\n",
      "320    1\n",
      "383    1\n",
      "183    1\n",
      "83     1\n",
      "82     1\n",
      "366    1\n",
      "141    1\n",
      "316    1\n",
      "337    1\n",
      "Name: y, dtype: int32\n",
      "74     0\n",
      "292    0\n",
      "345    0\n",
      "18     0\n",
      "324    0\n",
      "209    0\n",
      "306    0\n",
      "151    0\n",
      "31     0\n",
      "421    0\n",
      "180    0\n",
      "93     0\n",
      "194    0\n",
      "318    0\n",
      "443    1\n",
      "397    0\n",
      "247    0\n",
      "425    0\n",
      "171    0\n",
      "58     0\n",
      "444    0\n",
      "165    0\n",
      "198    0\n",
      "402    0\n",
      "323    0\n",
      "252    0\n",
      "322    0\n",
      "1      0\n",
      "390    0\n",
      "78     0\n",
      "Name: y, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# now that indices have been obtained we'll correspondingly slice y_train (to read the true labels) and poll to obtain\n",
    "# class for each cluster\n",
    "\n",
    "y_train_read_0 = y_train[sel_indx_0_cls]\n",
    "y_train_read_1 = y_train[sel_indx_1_cls]\n",
    "\n",
    "# slicing\n",
    "\n",
    "print(y_train_read_0)\n",
    "print(y_train_read_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# now polling to classify clusters to a class (0/1)\n",
    "\n",
    "import math\n",
    "pol_thrsh = 0.5 # this threshold will be varied for ROC and AUC later, code can be copied is pol_thrsh used\n",
    "                # threshold same for both clusters\n",
    "\n",
    "read_labels_0 = y_train_read_0.to_numpy().flatten()   # to numpy\n",
    "read_labels_1 = y_train_read_1.to_numpy().flatten()   # to numpy\n",
    "\n",
    "pol_num_0 = math.floor(pol_thrsh*read_labels_0.size)     # decisive polling number off of same thresholds\n",
    "pol_num_1 = math.floor(pol_thrsh*read_labels_1.size)     # decisive polling number off of same thresholds\n",
    "\n",
    "\n",
    "pol_cnt_c0_0 = 0                    # label 0 counted for cluster 0\n",
    "pol_cnt_c1_0 = 0                    # label 0 counted for cluster 1\n",
    "\n",
    "\n",
    "for i in np.arange(0,read_labels_0.size):\n",
    "    if read_labels_0[i] == 0:\n",
    "        pol_cnt_c0_0 = pol_cnt_c0_0 + 1\n",
    "        \n",
    "\n",
    "if pol_cnt_c0_0 > pol_num_0:\n",
    "    clus_0 = 0\n",
    "else:\n",
    "    clus_0 = 1\n",
    "    \n",
    "\n",
    "\n",
    "for i in np.arange(0,read_labels_1.size):\n",
    "    if read_labels_1[i] == 0:\n",
    "        pol_cnt_c1_0 = pol_cnt_c1_0 + 1\n",
    "        \n",
    "\n",
    "if pol_cnt_c1_0 > pol_num_1:\n",
    "    clus_1 = 0\n",
    "else:\n",
    "    clus_1 = 1\n",
    "    \n",
    "\n",
    "    \n",
    "print(clus_0)\n",
    "print(clus_1)\n",
    "\n",
    "\n",
    "# clus_0 and clus_1 assigns final class labels to both the cluster, via polling, using pol_thrsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1\n",
      " 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1\n",
      " 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# kmeans has been fit over X_train\n",
    "# by label reading, clusters 0 and 1 have been assigned labels (class) as described in HW\n",
    "# now this same kmeans fit (stays same throughout, obviously), once it predicts a cluster for a sample\n",
    "# can now predict class based on this cluster-class assigned, bridged via clus_0 and clus_1 which\n",
    "# is specific to our kmeans fit (obtained in last of our 10 iterations, is the same throughout)\n",
    "# all this, again, is for single Monte-Iteration, to get a 'feel'\n",
    "\n",
    "\n",
    "# NOW for various scores\n",
    "\n",
    "y_train_pred = np.zeros(X_train.shape[0])\n",
    "\n",
    "for i in np.arange(0,X_train.shape[0]):\n",
    "    if y_train_pred_cluster[i] == 0:\n",
    "        y_train_pred[i] = clus_0\n",
    "    else:\n",
    "        y_train_pred[i] = clus_1\n",
    "        \n",
    "y_train_pred = y_train_pred.astype(int)\n",
    "print(y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is : \n",
      " 14.977973568281937 %\n"
     ]
    }
   ],
   "source": [
    "mis = 0\n",
    "for l in np.arange(0,X_train.shape[0]):\n",
    "    if y_train_pred[l] != y_train.iloc[l]:\n",
    "        mis = mis + 1\n",
    "\n",
    "train_error = (mis/X_train.shape[0])*100\n",
    "print('The training error is : \\n',train_error,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix for training is : \n",
      " [[284   1]\n",
      " [ 67 102]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train,y_train_pred)\n",
    "\n",
    "print('The confusion matrix for training is : \\n',confusion_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix for train, appropriately indexed is : \n",
      "             Predicted 0  Predicted 1\n",
      "Actually 0          284            1\n",
      "Actually 1           67          102\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_train_df = pd.DataFrame(confusion_matrix_train,index=['Actually 0','Actually 1'],columns=['Predicted 0','Predicted 1'])\n",
    "print('The confusion matrix for train, appropriately indexed is : \\n',confusion_matrix_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for train is : \n",
      " 99.02912621359224 %\n",
      "The recall for train is : \n",
      " 60.35502958579882 %\n"
     ]
    }
   ],
   "source": [
    "precision_train = (confusion_matrix_train[1][1]/(confusion_matrix_train[1][1]+confusion_matrix_train[0][1]))*100\n",
    "recall_train = (confusion_matrix_train[1][1]/(confusion_matrix_train[1][1]+confusion_matrix_train[1][0]))*100\n",
    "\n",
    "print('The precision for train is : \\n',precision_train,'%')\n",
    "print('The recall for train is : \\n',recall_train,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for training is : \n",
      " 85.02202643171806 %\n",
      "The f1 score for training is : \n",
      " 0.7499999999999999\n"
     ]
    }
   ],
   "source": [
    "accuracy_train = ((confusion_matrix_train[0][0]+confusion_matrix_train[1][1])/(confusion_matrix_train[0][0]+confusion_matrix_train[1][1]+confusion_matrix_train[0][1]+confusion_matrix_train[1][0]))*100\n",
    "f1_train = ((2*(precision_train/100)*(recall_train/100))/((precision_train/100)+(recall_train/100))) # divided by 100 as precision and recall specified in percentage\n",
    "print('The accuracy for training is : \\n',accuracy_train,'%')\n",
    "print('The f1 score for training is : \\n',f1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1\n",
      " 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_cluster = kmeans.predict(X_test)\n",
    "\n",
    "print(y_test_pred_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0\n",
      " 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.zeros(X_test.shape[0])\n",
    "for i in np.arange(0,X_test.shape[0]):\n",
    "    if y_test_pred_cluster[i] == 0:\n",
    "        y_test_pred[i] = clus_0\n",
    "    else:\n",
    "        y_test_pred[i] = clus_1\n",
    "\n",
    "y_test_pred = y_test_pred.astype(int)\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test error is obtained : \n",
      " 15.65217391304348 %\n"
     ]
    }
   ],
   "source": [
    "mis = 0\n",
    "for l in np.arange(0,X_test.shape[0]):\n",
    "    if y_test_pred[l] != y_test.iloc[l]:\n",
    "        mis = mis + 1\n",
    "\n",
    "test_error = (mis/X_test.shape[0])*100\n",
    "print('The test error is obtained : \\n',test_error,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix for test is : \n",
      " [[72  0]\n",
      " [18 25]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_test = confusion_matrix(y_test,y_test_pred)\n",
    "\n",
    "print('The confusion matrix for test is : \\n',confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix for test, appropriately indexed is : \n",
      "             Predicted 0  Predicted 1\n",
      "Actually 0           72            0\n",
      "Actually 1           18           25\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_test_df = pd.DataFrame(confusion_matrix_test,index=['Actually 0','Actually 1'],columns=['Predicted 0','Predicted 1'])\n",
    "print('The confusion matrix for test, appropriately indexed is : \\n',confusion_matrix_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for test is : \n",
      " 100.0 %\n",
      "The recall for test is : \n",
      " 58.139534883720934 %\n"
     ]
    }
   ],
   "source": [
    "precision_test = (confusion_matrix_test[1][1]/(confusion_matrix_test[1][1]+confusion_matrix_test[0][1]))*100\n",
    "recall_test = (confusion_matrix_test[1][1]/(confusion_matrix_test[1][1]+confusion_matrix_test[1][0]))*100\n",
    "\n",
    "print('The precision for test is : \\n',precision_test,'%')\n",
    "print('The recall for test is : \\n',recall_test,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for test is : \n",
      " 84.34782608695653 %\n",
      "The f1 score for test is : \n",
      " 0.7352941176470588\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = ((confusion_matrix_test[0][0]+confusion_matrix_test[1][1])/(confusion_matrix_test[0][0]+confusion_matrix_test[1][1]+confusion_matrix_test[0][1]+confusion_matrix_test[1][0]))*100\n",
    "f1_test = ((2*(precision_test/100)*(recall_test/100))/((precision_test/100)+(recall_test/100))) # divided by 100 as precision and recall specified in percentage\n",
    "print('The accuracy for test is : \\n',accuracy_test,'%')\n",
    "print('The f1 score for test is : \\n',f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC and AUC for Train and Test\n",
    "\n",
    "Again, the code of pol_thrsh will be used here to obtain to ROC and AUC, both, for test as well as for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             FPR      TPR\n",
      "0.00  0.00350877  0.60355\n",
      "0.01  0.00350877  0.60355\n",
      "0.02  0.00350877  0.60355\n",
      "0.03  0.00350877  0.60355\n",
      "0.04  0.00350877  0.60355\n",
      "...          ...      ...\n",
      "0.96  0.00350877  0.60355\n",
      "0.97           1        1\n",
      "0.98           1        1\n",
      "0.99           1        1\n",
      "1.00           1        1\n",
      "\n",
      "[101 rows x 2 columns]\n",
      "     FPR       TPR\n",
      "0.00   0  0.581395\n",
      "0.01   0  0.581395\n",
      "0.02   0  0.581395\n",
      "0.03   0  0.581395\n",
      "0.04   0  0.581395\n",
      "...   ..       ...\n",
      "0.96   0  0.581395\n",
      "0.97   1         1\n",
      "0.98   1         1\n",
      "0.99   1         1\n",
      "1.00   1         1\n",
      "\n",
      "[101 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# FOR TRAIN FIRST\n",
    "\n",
    "# read_labels_0 and read_labels_1 stay consequential\n",
    "# kmeans is consequential and predicted clusters (for both train and test) stay the same (as kmeans is the same)\n",
    "# class prediction using the predicted clusters depend on our 'polling'\n",
    "# polling depends on threshold which for roc and auc will be varied\n",
    "# thus y_train_pred_cluster and y_test_pred_cluster stay same\n",
    "# classifying rule changes\n",
    "# so y_train_pred and y_test_pred changes\n",
    "# this gives the roc and auc\n",
    "\n",
    "pol_thrsh_ra_range = np.arange(0,1.01,0.01)  # varying threshold\n",
    "ROC_df = pd.DataFrame(index=pol_thrsh_ra_range,columns=['FPR','TPR'])\n",
    "ROC_df_test = pd.DataFrame(index=pol_thrsh_ra_range,columns=['FPR','TPR'])\n",
    "\n",
    "\n",
    "\n",
    "for ra in np.arange(0,pol_thrsh_ra_range.size):\n",
    "    \n",
    "    pol_thrsh_ra = pol_thrsh_ra_range[ra]\n",
    "    \n",
    "    pol_num_0_ra = math.floor(pol_thrsh_ra*read_labels_0.size)    \n",
    "    pol_num_1_ra = math.floor(pol_thrsh_ra*read_labels_1.size)\n",
    "    \n",
    "    pol_cnt_c0_0_ra = 0                    \n",
    "    pol_cnt_c1_0_ra = 0                    \n",
    "\n",
    "\n",
    "    for i in np.arange(0,read_labels_0.size):\n",
    "        if read_labels_0[i] == 0:\n",
    "            pol_cnt_c0_0_ra = pol_cnt_c0_0_ra + 1\n",
    "\n",
    "\n",
    "    if pol_cnt_c0_0_ra > pol_num_0_ra:\n",
    "        clus_0_ra = 0\n",
    "    else:\n",
    "        clus_0_ra = 1\n",
    "\n",
    "\n",
    "\n",
    "    for i in np.arange(0,read_labels_1.size):\n",
    "        if read_labels_1[i] == 0:\n",
    "            pol_cnt_c1_0_ra = pol_cnt_c1_0_ra + 1\n",
    "\n",
    "\n",
    "    if pol_cnt_c1_0_ra > pol_num_1_ra:\n",
    "        clus_1_ra = 0\n",
    "    else:\n",
    "        clus_1_ra = 1\n",
    "\n",
    "\n",
    "\n",
    "    # local cluster to class fixation done, based of given polling threshold\n",
    "    \n",
    "    y_train_pred_ra = np.zeros(X_train.shape[0])\n",
    "\n",
    "    for i in np.arange(0,X_train.shape[0]):\n",
    "        if y_train_pred_cluster[i] == 0:\n",
    "            y_train_pred_ra[i] = clus_0_ra\n",
    "        else:\n",
    "            y_train_pred_ra[i] = clus_1_ra\n",
    "\n",
    "    y_train_pred_ra = y_train_pred_ra.astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    y_test_pred_ra = np.zeros(X_test.shape[0])\n",
    "    for i in np.arange(0,X_test.shape[0]):\n",
    "        if y_test_pred_cluster[i] == 0:\n",
    "            y_test_pred_ra[i] = clus_0_ra\n",
    "        else:\n",
    "            y_test_pred_ra[i] = clus_1_ra\n",
    "\n",
    "    y_test_pred_ra = y_test_pred_ra.astype(int)\n",
    "    \n",
    "    # _ra everywhere to ensure distinction and localisation to this ROC and AUC section\n",
    "    # this avoids any variable conflicts\n",
    "    \n",
    "    \n",
    "    confusion_matrix_train_ra = confusion_matrix(y_train,y_train_pred_ra)\n",
    "    ROC_df.iloc[ra,0] = confusion_matrix_train_ra[0][1]/(confusion_matrix_train_ra[0][1]+confusion_matrix_train_ra[0][0])\n",
    "    ROC_df.iloc[ra,1] = confusion_matrix_train_ra[1][1]/(confusion_matrix_train_ra[1][1]+confusion_matrix_train_ra[1][0])\n",
    "    \n",
    "    \n",
    "    confusion_matrix_test_ra = confusion_matrix(y_test,y_test_pred_ra)\n",
    "    ROC_df_test.iloc[ra,0] = confusion_matrix_test_ra[0][1]/(confusion_matrix_test_ra[0][1]+confusion_matrix_test_ra[0][0])\n",
    "    ROC_df_test.iloc[ra,1] = confusion_matrix_test_ra[1][1]/(confusion_matrix_test_ra[1][1]+confusion_matrix_test_ra[1][0])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "print(ROC_df)\n",
    "print(ROC_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'TPR for Train')"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ROC_df['FPR'],ROC_df['TPR'])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('FPR for Train')\n",
    "plt.ylabel('TPR for Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'TPR for Test')"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ROC_df_test['FPR'],ROC_df_test['TPR'])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('FPR for Test')\n",
    "plt.ylabel('TPR for Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for train is : \n",
      " 0.7989619017959099\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "AUC_train = metrics.auc(ROC_df['FPR'],ROC_df['TPR'])\n",
    "print('The AUC for train is : \\n',AUC_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for test is : \n",
      " 0.7906976744186047\n"
     ]
    }
   ],
   "source": [
    "AUC_test = metrics.auc(ROC_df_test['FPR'],ROC_df_test['TPR'])\n",
    "print('The AUC for test is : \\n',AUC_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to pol_thrsh algorithm that uses floor function, a threshold that should correspond to origin, corresponds very close\n",
    "to origin, but not quite\n",
    "\n",
    "Thus, this is why the ROC curves don't start from origin as is generally expected\n",
    "\n",
    "As such, one reason for this also how polling for kmeans is done\n",
    "\n",
    "This doesn't really affect the evaluational parameters, and the AUC\n",
    "\n",
    "Due to polling nature and due to how thresholds were defined, and how 'sensitive' kmeans polling is to these thresholds the ROC curves are obtained in these consequential fashion\n",
    "\n",
    "Again, this doesn't affect our consequential interpretations or the actual AUC which is obtained\n",
    "\n",
    "As such the curves are in fact sharp (were obtained from origin previously; by minor polling variation) and we can assume the initial portion to coincide with the y-axis, like it is here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
