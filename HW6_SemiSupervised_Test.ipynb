{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW6\n",
    "Semi-Supervised Test (M=1) iteration\n",
    "\n",
    "Serves the same purpose as for Supervised case, that is,\n",
    "To get a feel of the code workings\n",
    "To perform the single time tasks\n",
    "Looping for Monte Carlo with M=30 after performing once is rather easy\n",
    "\n",
    "\n",
    "STEPS for a SINGLE ITERATION (M=1)\n",
    "\n",
    "- Split into train and test randomly\n",
    "- Split train into L and U (according to required proportions) (ignore labels of U) randomly\n",
    "- Stick, for a single iteration, with the obtained L, U, test (obvious; restating for relevance)\n",
    "- Train on L using CV (5-fold) to select C, Penalty\n",
    "- Once C selected over L, keep it constant for that Monte-Carlo iteration      (For C)\n",
    "\n",
    "- Once C selected, retrain over entire L. Classify U.                           (1)\n",
    "- Select point in U farthest from SVM hyperplane (current SVM's) (SVM trained in 1)\n",
    "- Classify using (1), this point in U\n",
    "- Update L (add this point to L)                                     (L')\n",
    "- Update U (remove this point from U)                                (U')\n",
    "- Go back to (1) and repeat till U is empty\n",
    "\n",
    "\n",
    "- Once U is empty, retrain on the final 'whole' L, with C obtained in (For C)\n",
    "- Use this SVM for all the scores over train and test (Final SVM)\n",
    "\n",
    "\n",
    "\n",
    "For Monte Carlo, repeat above procedure for M=30 times\n",
    "\n",
    "ASSUMPTIONS :\n",
    "Ideally after L and U updation (L' and U') we should reselect C again using 5-fold CV. Obviously if train originally had, say, 400 points, L initial and U initial = ~200\n",
    "\n",
    "So L and U updated 200 times. Cross Validating again over C is extremely expensive, every single time. It does not make sense especially if M=30, run time will be extremely large.\n",
    "\n",
    "Two solutions to this :\n",
    "1. Count total updations that will be performed. At 'interim-intervals' over these counts, recalibrate for new C, Penalty\n",
    "2. Stick with the C obtained in (For C)\n",
    "\n",
    "As such, option 1. seems accurate, as it does the following :\n",
    "\n",
    "- L updations are ~200, say\n",
    "- It keeps a counter of updations\n",
    "- Say after every 50 updations it recalibrates for new C, so C will be selected 4 times after being initially selected\n",
    "\n",
    "Also option 1. is not computationally taxing, however it will increase the run times heavily.\n",
    "\n",
    "We will thus, \n",
    "STICK to the C initially selected. Not the most accurate, we assume it is still appropriate especially with the run time constraints. Thus we use option 2. and the algorithm given for the single iteration is also for this 'lenient-straight-forward' option\n",
    "\n",
    "Additionally, as always, for simplicity, we assume Normalization happens only ONCE, before all these steps, in a 'given normalized dataset' sense. Pipelineing for Normalizations shouldn't be difficult and would require 1-2 additional line of codes, however we presume that for simplicity and to see 'the actual algorithm we are performing', a pre-normalization is done initially.\n",
    "\n",
    "In line with these assumptions and algorithms, we perform Semi-Supervised Learning, A. and B. for a single iteration, looping subsequently in the next section the single iteration code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSUMPTION FOR RANGE SELECTION OF C\n",
    "Also Range Selection of C won't be done again, the same range will be used again, here, as L1 Linear SVC is used again here\n",
    "While, it ideally should be selected again for this particular example, we will assume, the same range is valid enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           X1        X2        X3        X4        X5        X6        X7  \\\n",
      "0    0.012420  0.018966  0.081663  0.509882  0.000097  0.000134  0.000092   \n",
      "1    0.016894  0.018418  0.109595  0.631681  0.000148  0.000132  0.000040   \n",
      "2    0.018141  0.037717  0.117454  0.635971  0.000146  0.000161  0.000175   \n",
      "3    0.010303  0.012682  0.067519  0.554766  0.000068  0.000064  0.000091   \n",
      "4    0.016205  0.013103  0.105288  0.669741  0.000155  0.000125  0.000082   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "221  0.010487  0.015225  0.070246  0.520099  0.000077  0.000116  0.000142   \n",
      "222  0.008548  0.012580  0.058416  0.527754  0.000040  0.000090  0.000078   \n",
      "223  0.019560  0.037239  0.126930  0.629899  0.000198  0.000235  0.000154   \n",
      "224  0.014801  0.022077  0.096247  0.571976  0.000118  0.000126  0.000095   \n",
      "225  0.017363  0.019220  0.111276  0.617599  0.000134  0.000113  0.000063   \n",
      "\n",
      "           X8        X9       X10  ...       X21       X22       X23  \\\n",
      "0    0.000058  0.000185  0.000063  ...  0.016254  0.026186  0.105773   \n",
      "1    0.000040  0.000261  0.000090  ...  0.018488  0.024682  0.126641   \n",
      "2    0.000065  0.000219  0.000104  ...  0.019670  0.058569  0.129684   \n",
      "3    0.000060  0.000104  0.000037  ...  0.012676  0.020020  0.086482   \n",
      "4    0.000079  0.000288  0.000080  ...  0.016863  0.014959  0.109420   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "221  0.000063  0.000139  0.000049  ...  0.013418  0.020270  0.086595   \n",
      "222  0.000050  0.000083  0.000027  ...  0.010953  0.015859  0.078092   \n",
      "223  0.000052  0.000305  0.000125  ...  0.021350  0.046042  0.139732   \n",
      "224  0.000045  0.000228  0.000076  ...  0.017768  0.029246  0.114004   \n",
      "225  0.000050  0.000242  0.000099  ...  0.019549  0.025686  0.124178   \n",
      "\n",
      "          X24       X25       X26       X27       X28       X29       X30  \n",
      "0    0.848431  0.000143  0.000348  0.000343  0.000140  0.000350  0.000096  \n",
      "1    0.755578  0.000198  0.000358  0.000137  0.000113  0.000481  0.000128  \n",
      "2    0.747404  0.000205  0.000397  0.000572  0.000152  0.000333  0.000138  \n",
      "3    0.822546  0.000088  0.000124  0.000237  0.000110  0.000158  0.000046  \n",
      "4    0.725193  0.000174  0.000165  0.000102  0.000089  0.000325  0.000086  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "221  0.845344  0.000123  0.000327  0.000489  0.000135  0.000218  0.000083  \n",
      "222  0.842133  0.000056  0.000233  0.000186  0.000085  0.000123  0.000044  \n",
      "223  0.749524  0.000244  0.000466  0.000463  0.000157  0.000476  0.000159  \n",
      "224  0.804613  0.000169  0.000282  0.000317  0.000120  0.000358  0.000104  \n",
      "225  0.767134  0.000214  0.000277  0.000288  0.000126  0.000415  0.000131  \n",
      "\n",
      "[226 rows x 30 columns]\n",
      "0      1\n",
      "1      0\n",
      "2      0\n",
      "3      1\n",
      "4      0\n",
      "      ..\n",
      "221    1\n",
      "222    1\n",
      "223    0\n",
      "224    0\n",
      "225    0\n",
      "Name: y, Length: 226, dtype: int32\n",
      "           X1        X2        X3        X4        X5        X6        X7  \\\n",
      "0    0.017107  0.019741  0.110377  0.647296  0.000114  0.000110  0.000096   \n",
      "1    0.014786  0.020707  0.096836  0.622733  0.000098  0.000124  0.000105   \n",
      "2    0.016447  0.023429  0.107238  0.645270  0.000136  0.000164  0.000121   \n",
      "3    0.007925  0.004573  0.054099  0.440986  0.000052  0.000122  0.000132   \n",
      "4    0.017099  0.032057  0.109290  0.597868  0.000132  0.000099  0.000077   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "223  0.015323  0.024507  0.098404  0.577592  0.000120  0.000101  0.000052   \n",
      "224  0.006925  0.008892  0.045750  0.376233  0.000047  0.000061  0.000063   \n",
      "225  0.008960  0.014242  0.058332  0.488324  0.000052  0.000058  0.000053   \n",
      "226  0.009883  0.006985  0.065808  0.631774  0.000049  0.000065  0.000096   \n",
      "227  0.016996  0.024027  0.107755  0.683236  0.000117  0.000049  0.000033   \n",
      "\n",
      "           X8        X9       X10  ...       X21       X22       X23  \\\n",
      "0    0.000036  0.000231  0.000086  ...  0.018396  0.027027  0.122804   \n",
      "1    0.000052  0.000200  0.000067  ...  0.016388  0.027602  0.109760   \n",
      "2    0.000071  0.000256  0.000082  ...  0.017884  0.031318  0.118745   \n",
      "3    0.000065  0.000107  0.000035  ...  0.011181  0.007635  0.081325   \n",
      "4    0.000029  0.000224  0.000098  ...  0.019617  0.043954  0.126646   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "223  0.000051  0.000248  0.000075  ...  0.018092  0.035533  0.115456   \n",
      "224  0.000038  0.000083  0.000026  ...  0.011005  0.013126  0.073616   \n",
      "225  0.000034  0.000094  0.000033  ...  0.012044  0.018093  0.079787   \n",
      "226  0.000051  0.000088  0.000029  ...  0.010979  0.008120  0.074137   \n",
      "227  0.000038  0.000191  0.000076  ...  0.017348  0.029753  0.110168   \n",
      "\n",
      "          X24       X25       X26       X27       X28       X29       X30  \n",
      "0    0.742429  0.000144  0.000341  0.000431  0.000128  0.000375  0.000124  \n",
      "1    0.767240  0.000124  0.000343  0.000396  0.000152  0.000297  0.000096  \n",
      "2    0.745085  0.000169  0.000335  0.000324  0.000159  0.000392  0.000105  \n",
      "3    0.889462  0.000071  0.000293  0.000314  0.000117  0.000203  0.000052  \n",
      "4    0.781225  0.000256  0.000331  0.000470  0.000125  0.000427  0.000133  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "223  0.799858  0.000179  0.000256  0.000173  0.000135  0.000334  0.000102  \n",
      "224  0.921243  0.000075  0.000149  0.000195  0.000087  0.000150  0.000040  \n",
      "225  0.864495  0.000084  0.000128  0.000138  0.000082  0.000158  0.000048  \n",
      "226  0.767189  0.000067  0.000100  0.000195  0.000079  0.000115  0.000037  \n",
      "227  0.712063  0.000127  0.000060  0.000063  0.000065  0.000259  0.000080  \n",
      "\n",
      "[228 rows x 30 columns]\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      1\n",
      "4      0\n",
      "      ..\n",
      "223    0\n",
      "224    1\n",
      "225    1\n",
      "226    1\n",
      "227    0\n",
      "Name: y, Length: 228, dtype: int32\n",
      "           X1        X2        X3        X4        X5        X6        X7  \\\n",
      "0    0.015166  0.019528  0.097151  0.632211  0.000110  0.000088  0.000050   \n",
      "1    0.009751  0.009202  0.064677  0.562586  0.000057  0.000088  0.000103   \n",
      "2    0.015056  0.015967  0.095422  0.612151  0.000096  0.000043  0.000005   \n",
      "3    0.013718  0.013633  0.090182  0.668940  0.000084  0.000122  0.000056   \n",
      "4    0.014298  0.015030  0.093043  0.633785  0.000099  0.000101  0.000084   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "110  0.017292  0.025478  0.110228  0.631398  0.000122  0.000082  0.000039   \n",
      "111  0.016030  0.019653  0.103562  0.633316  0.000113  0.000103  0.000073   \n",
      "112  0.013871  0.012063  0.089169  0.649979  0.000073  0.000063  0.000031   \n",
      "113  0.014646  0.022767  0.096758  0.659609  0.000085  0.000134  0.000103   \n",
      "114  0.009230  0.013142  0.062774  0.566806  0.000053  0.000124  0.000157   \n",
      "\n",
      "           X8        X9       X10  ...       X21       X22       X23  \\\n",
      "0    0.000035  0.000192  0.000067  ...  0.016561  0.026048  0.106029   \n",
      "1    0.000053  0.000100  0.000032  ...  0.011931  0.011147  0.078741   \n",
      "2    0.000010  0.000168  0.000064  ...  0.016994  0.020074  0.108401   \n",
      "3    0.000046  0.000169  0.000056  ...  0.014397  0.016238  0.095951   \n",
      "4    0.000055  0.000171  0.000060  ...  0.015694  0.019654  0.107529   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "110  0.000033  0.000274  0.000083  ...  0.019057  0.037575  0.123084   \n",
      "111  0.000038  0.000213  0.000074  ...  0.017604  0.024176  0.116468   \n",
      "112  0.000024  0.000157  0.000051  ...  0.014793  0.014364  0.095424   \n",
      "113  0.000038  0.000146  0.000062  ...  0.015539  0.027374  0.106304   \n",
      "114  0.000068  0.000107  0.000031  ...  0.011533  0.017663  0.082713   \n",
      "\n",
      "          X24       X25       X26       X27       X28       X29       X30  \n",
      "0    0.759959  0.000133  0.000149  0.000193  0.000100  0.000279  0.000077  \n",
      "1    0.819306  0.000088  0.000266  0.000384  0.000124  0.000199  0.000048  \n",
      "2    0.775745  0.000117  0.000067  0.000021  0.000041  0.000243  0.000076  \n",
      "3    0.730871  0.000105  0.000216  0.000179  0.000106  0.000267  0.000076  \n",
      "4    0.759334  0.000130  0.000271  0.000307  0.000158  0.000266  0.000076  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "110  0.755577  0.000200  0.000257  0.000192  0.000133  0.000452  0.000102  \n",
      "111  0.756881  0.000166  0.000326  0.000434  0.000122  0.000376  0.000097  \n",
      "112  0.747960  0.000103  0.000159  0.000124  0.000075  0.000227  0.000062  \n",
      "113  0.736301  0.000103  0.000318  0.000368  0.000111  0.000227  0.000080  \n",
      "114  0.815932  0.000074  0.000389  0.000421  0.000119  0.000183  0.000056  \n",
      "\n",
      "[115 rows x 30 columns]\n",
      "0      0\n",
      "1      1\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "110    0\n",
      "111    0\n",
      "112    0\n",
      "113    0\n",
      "114    1\n",
      "Name: y, Length: 115, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('wdbc.csv', header = None)\n",
    "\n",
    "\n",
    "output = df.iloc[:,0]\n",
    "\n",
    "\n",
    "for i in np.arange(0,df.shape[0]):\n",
    "    if output.iloc[i] == 'B':\n",
    "        output.iloc[i] = 0\n",
    "    else:\n",
    "        output.iloc[i] = 1\n",
    "\n",
    "\n",
    "\n",
    "features = df.iloc[:,1:]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "features_normal = normalize(features)\n",
    "\n",
    "\n",
    "features_normal_df = pd.DataFrame(features_normal)\n",
    "\n",
    "\n",
    "norm_whole = pd.concat([features_normal_df,output],axis=1)\n",
    "\n",
    "\n",
    "col_head = norm_whole.columns\n",
    "\n",
    "\n",
    "norm_whole.columns= ['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13','X14','X15','X16','X17','X18','X19','X20','X21','X22','X23','X24','X25','X26','X27','X28','X29','X30','y']\n",
    "\n",
    "\n",
    "norm_whole.sort_values(by=['y'],inplace=True)\n",
    "\n",
    "\n",
    "norm_whole.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "X_y_0 = norm_whole.iloc[0:357,:]\n",
    "X_y_1 = norm_whole.iloc[357:569,:]\n",
    "X_y_1.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_y_train_0,X_y_test_0 = train_test_split(X_y_0,test_size = 0.2,shuffle=True)\n",
    "X_y_train_1,X_y_test_1 = train_test_split(X_y_1,test_size = 0.2,shuffle=True)\n",
    "\n",
    "X_y_train_0_l, X_y_train_0_u = train_test_split(X_y_train_0,test_size = 0.5, shuffle=True) \n",
    "X_y_train_1_l, X_y_train_1_u = train_test_split(X_y_train_1,test_size = 0.5, shuffle=True)\n",
    "\n",
    "\n",
    "X_y_train = pd.concat([X_y_train_0_l,X_y_train_1_l],axis=0)\n",
    "X_y_train_u = pd.concat([X_y_train_0_u,X_y_train_1_u],axis=0)\n",
    "X_y_test = pd.concat([X_y_test_0,X_y_test_1],axis=0)\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_y_train = shuffle(X_y_train)\n",
    "X_y_train_u = shuffle(X_y_train_u)\n",
    "X_y_test = shuffle(X_y_test)\n",
    "\n",
    "\n",
    "\n",
    "X_y_train.reset_index(drop=True,inplace=True)\n",
    "X_y_train_u.reset_index(drop=True,inplace=True)\n",
    "X_y_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "X_train = X_y_train.drop(columns=['y'])\n",
    "X_train_u = X_y_train_u.drop(columns=['y'])\n",
    "X_test = X_y_test.drop(columns=['y'])\n",
    "y_train = X_y_train['y'].astype(int)\n",
    "y_train_u = X_y_train_u['y'].astype(int)\n",
    "y_test =  X_y_test['y'].astype(int)\n",
    "print(X_train)                                    # Labeled Train\n",
    "print(y_train)\n",
    "print(X_train_u)                                  # Unlabeled Train\n",
    "print(y_train_u)\n",
    "print(X_test)                                     # Test\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Estimated Test Errors\n",
      "0.1                      37.1787\n",
      "1.0                      10.1449\n",
      "10.0                     10.6377\n",
      "100.0                    8.84058\n",
      "1000.0                   5.31401\n",
      "10000.0                  6.14493\n",
      "100000.0                 6.63768\n",
      "1000000.0                5.31401\n",
      "10000000.0               7.54589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.svm import LinearSVC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  \n",
    "\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "\n",
    "C = np.array([10**(-1),10**(0),10**(1),10**(2),10**(3),10**(4),10**(5),10**(6),10**(7)]) \n",
    "\n",
    "error_df = pd.DataFrame(index = C, columns = ['Estimated Test Errors'])\n",
    "\n",
    "for i in np.arange(0,C.size):\n",
    "    \n",
    "    cv_ite = -1\n",
    "    cv_error_vector = np.zeros(5)  \n",
    "    \n",
    "    for train,test in kf.split(X_train):\n",
    "        \n",
    "        cv_ite = cv_ite + 1  \n",
    "        X_train_cv,X_test_cv = X_train.iloc[train,:],X_train.iloc[test,:]\n",
    "        y_train_cv,y_test_cv = y_train[train],y_train[test]\n",
    "        clf = LinearSVC(penalty='l1',dual=False,C=C[i]).fit(X_train_cv,y_train_cv)\n",
    "        y_test_cv_pred = clf.predict(X_test_cv)\n",
    "        \n",
    "        mis = 0\n",
    "        for l in np.arange(0,X_test_cv.shape[0]):\n",
    "            if y_test_cv_pred[l] != y_test_cv.iloc[l]:\n",
    "                mis = mis + 1\n",
    "        \n",
    "        percent_error = (mis/X_test_cv.shape[0])*100\n",
    "        cv_error_vector[cv_ite] = percent_error\n",
    "        \n",
    "    \n",
    "    error_df.iloc[i,0] = np.mean(cv_error_vector)\n",
    "\n",
    "\n",
    "print(error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum test error estimate obtained is thus : \n",
      " 5.314009661835749 %\n",
      "The C for which this minimum test error is obtained is thus : \n",
      " 1000.0\n"
     ]
    }
   ],
   "source": [
    "min_error = np.min(error_df.to_numpy().flatten())\n",
    "min_C = C[np.argmin(error_df.to_numpy().flatten())]\n",
    "\n",
    "print('The minimum test error estimate obtained is thus : \\n',min_error,'%')\n",
    "print('The C for which this minimum test error is obtained is thus : \\n',min_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the C selection, which will be kept throughout, till the end\n",
    "\n",
    "This C selected by cross validating on X_train and y_train which are the initial labeled points\n",
    "\n",
    "NOW :\n",
    "\n",
    "Retraining using this C on all labeled points\n",
    "Then un-labeled (train) farthest from SVM point selection, and addition to 'labeled train'\n",
    "Retraining and reiterating till unlabeled set is empty\n",
    "\n",
    "This portion is done as follows, in the following coding section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing, if required later\n",
    "\n",
    "X_train_O = X_train.copy()\n",
    "y_train_O = y_train.copy()\n",
    "X_train_u_O = X_train_u.copy()\n",
    "y_train_u_O = y_train_u.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel_length = X_train_u.shape[0]\n",
    "\n",
    "\n",
    "while unlabel_length > 0:\n",
    "    \n",
    "    clf_u = LinearSVC(penalty='l1',dual=False,C=min_C).fit(X_train,y_train)   # whole labelled training sets, C same as assumed\n",
    "    y_train_u_pred = clf_u.predict(X_train_u)\n",
    "    X_train_u_decision_function = clf_u.decision_function(X_train_u)    # calculates signed distances from hyperplane\n",
    "    X_train_u_distance = abs(X_train_u_decision_function)               # magnitudinal distances\n",
    "    unlab_samp_selec_indx = np.argmax(X_train_u_distance)               # returns sample index selected\n",
    "    unlab_samp_selec = X_train_u.iloc[unlab_samp_selec_indx,:].to_numpy().reshape(1,X_train_u.shape[1])\n",
    "    unlab_samp_selec_df = pd.DataFrame(unlab_samp_selec, columns=X_train_u.columns)\n",
    "    y_unlab_samp_selec = y_train_u_pred[unlab_samp_selec_indx] # can be also obtained via decision function, as such\n",
    "    X_train = pd.concat([X_train,unlab_samp_selec_df],axis=0)\n",
    "    X_train.reset_index(drop=True,inplace=True)       # updated X train with index reset\n",
    "    y_train[y_train.size] = y_unlab_samp_selec        # updated y train (appended), index external already 'consequential', no need to reset\n",
    "\n",
    "\n",
    "\n",
    "    # Assuming that once a sample is added from unlabeled set to labeled set, it can't be added again (not specified, so assumed)\n",
    "    # we redact the corresponding row from the unlabeled set\n",
    "\n",
    "    # all indices are internal indices, not external; they should be same as reset_index has been used post updations\n",
    "    # but to avoid any issues, dropping of rows won't be performed by using drop as it uses the external indices\n",
    "    # rather, indices will be internally referred via iloc and will exclude the index (internal) to be removed\n",
    "\n",
    "    indx_interim = np.arange(0,X_train_u.shape[0])\n",
    "    indx_selected = np.delete(indx_interim,unlab_samp_selec_indx)\n",
    "\n",
    "    X_train_u = X_train_u.iloc[indx_selected,:]\n",
    "    X_train_u.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    y_train_u = y_train_u.iloc[indx_selected]          # y_train_u has the labels, actual, for the 'unlabeled' data\n",
    "    y_train_u.reset_index(drop=True,inplace=True)      # not used as such, but we still update for consistency\n",
    "\n",
    "    unlabel_length = X_train_u.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           X1        X2        X3        X4        X5        X6        X7  \\\n",
      "0    0.012420  0.018966  0.081663  0.509882  0.000097  0.000134  0.000092   \n",
      "1    0.016894  0.018418  0.109595  0.631681  0.000148  0.000132  0.000040   \n",
      "2    0.018141  0.037717  0.117454  0.635971  0.000146  0.000161  0.000175   \n",
      "3    0.010303  0.012682  0.067519  0.554766  0.000068  0.000064  0.000091   \n",
      "4    0.016205  0.013103  0.105288  0.669741  0.000155  0.000125  0.000082   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "449  0.012479  0.013833  0.080621  0.633850  0.000073  0.000064  0.000044   \n",
      "450  0.014459  0.017661  0.094888  0.603899  0.000107  0.000158  0.000103   \n",
      "451  0.014931  0.025101  0.095164  0.696047  0.000094  0.000051  0.000024   \n",
      "452  0.017017  0.042856  0.109547  0.607358  0.000137  0.000125  0.000105   \n",
      "453  0.013732  0.022049  0.088052  0.575230  0.000083  0.000062  0.000032   \n",
      "\n",
      "           X8        X9       X10  ...       X21       X22       X23  \\\n",
      "0    0.000058  0.000185  0.000063  ...  0.016254  0.026186  0.105773   \n",
      "1    0.000040  0.000261  0.000090  ...  0.018488  0.024682  0.126641   \n",
      "2    0.000065  0.000219  0.000104  ...  0.019670  0.058569  0.129684   \n",
      "3    0.000060  0.000104  0.000037  ...  0.012676  0.020020  0.086482   \n",
      "4    0.000079  0.000288  0.000080  ...  0.016863  0.014959  0.109420   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "449  0.000037  0.000113  0.000042  ...  0.013712  0.019248  0.088638   \n",
      "450  0.000058  0.000220  0.000072  ...  0.016430  0.028093  0.108908   \n",
      "451  0.000029  0.000156  0.000055  ...  0.014931  0.025101  0.095164   \n",
      "452  0.000030  0.000263  0.000090  ...  0.019197  0.056785  0.125890   \n",
      "453  0.000021  0.000182  0.000057  ...  0.016276  0.030907  0.104727   \n",
      "\n",
      "          X24       X25       X26       X27       X28       X29       X30  \n",
      "0    0.848431  0.000143  0.000348  0.000343  0.000140  0.000350  0.000096  \n",
      "1    0.755578  0.000198  0.000358  0.000137  0.000113  0.000481  0.000128  \n",
      "2    0.747404  0.000205  0.000397  0.000572  0.000152  0.000333  0.000138  \n",
      "3    0.822546  0.000088  0.000124  0.000237  0.000110  0.000158  0.000046  \n",
      "4    0.725193  0.000174  0.000165  0.000102  0.000089  0.000325  0.000086  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "449  0.763101  0.000101  0.000127  0.000126  0.000069  0.000181  0.000049  \n",
      "450  0.782492  0.000152  0.000456  0.000484  0.000173  0.000379  0.000110  \n",
      "451  0.696047  0.000094  0.000051  0.000024  0.000029  0.000156  0.000055  \n",
      "452  0.772256  0.000206  0.000297  0.000428  0.000100  0.000422  0.000106  \n",
      "453  0.805016  0.000112  0.000209  0.000213  0.000114  0.000306  0.000073  \n",
      "\n",
      "[454 rows x 30 columns]\n",
      "0      1\n",
      "1      0\n",
      "2      0\n",
      "3      1\n",
      "4      0\n",
      "      ..\n",
      "449    1\n",
      "450    0\n",
      "451    1\n",
      "452    0\n",
      "453    0\n",
      "Name: y, Length: 454, dtype: int64\n",
      "Empty DataFrame\n",
      "Columns: [X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X25, X26, X27, X28, X29, X30]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 30 columns]\n",
      "Series([], Name: y, dtype: int32)\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_train_u)\n",
    "print(y_train_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the unlabeled sets have been updated to null, as we copied them one by one into the training sets\n",
    "NOTE : The 'actual labels' of unlabeled train were ignored and were infact predicted as we kept on iteratively training the modified trains\n",
    "\n",
    "Which variables contain what ?\n",
    "\n",
    "X_train ---------------- the final X train (obtained, as needed, by combining labeled and unlabeled train as asked in HW)\n",
    "y_train ----------------- the final y train (obtained, as needed, by combining labeled and unlabeled train as asked in HW)\n",
    "X_train_u ---------------- empty (symbolizes that all unlabeled entities were taken care of)\n",
    "y_train_u ----------------- empty (similar meaning)\n",
    "\n",
    "X_train_O ------------- original X_train (labeled)                   in the current iteration                          \n",
    "y_train_O -------------- original y_train (labeled)                  in the current iteration\n",
    "X_train_u_O ------------- original X_train (unlabeled)               in the current iteration\n",
    "y_train_u_O -------------- original y_train (unlabeled)*             in the current iteration\n",
    "\n",
    "*actual labels of samples taken as 'unlabeled'\n",
    "\n",
    "The original copies were saved, just in case their need arises\n",
    "\n",
    "NOTE : After the last unlabeled sample was added to train consequently, the unlabel_length becomes 0 and we come out of the loop\n",
    "\n",
    "Thus the FINAL TRAINING on the final X train and y train is left\n",
    "\n",
    "NOW :\n",
    "We will train these final X train and y train\n",
    "And obtain all the scores that are relevant for us\n",
    "\n",
    "Training Deliberations :\n",
    "Training errors will be obtained over the generated y train. Here the actual labels have been given, however in most scenarios they are actually unknown. Thus the labels estimated for these unlabeled training points, will be used to obtain the training errors, rather than obtaining the training errors over the actual labels of the unlabeled data, as, again, they are only known here and are in fact unknown.\n",
    "\n",
    "THUS for training errors the generated y train will be used, rather than the original y train. \n",
    "\n",
    "(as the actual labels of the unlabeled data are almost always unknown, and are only available here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual test error, for minimizing C is obtained : \n",
      " 2.608695652173913 %\n"
     ]
    }
   ],
   "source": [
    "clf_final = LinearSVC(penalty='l1',dual=False,C=min_C).fit(X_train,y_train)\n",
    "y_test_pred = clf_final.predict(X_test)\n",
    "\n",
    "mis = 0\n",
    "for l in np.arange(0,X_test.shape[0]):\n",
    "    if y_test_pred[l] != y_test.iloc[l]:\n",
    "        mis = mis + 1\n",
    "\n",
    "test_error = (mis/X_test.shape[0])*100\n",
    "print('The actual test error, for minimizing C is obtained : \\n',test_error,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error, for minimizing C is obtained : \n",
      " 1.9823788546255507 %\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = clf_final.predict(X_train)\n",
    "\n",
    "mis = 0\n",
    "for l in np.arange(0,X_train.shape[0]):\n",
    "    if y_train_pred[l] != y_train.iloc[l]:\n",
    "        mis = mis + 1\n",
    "\n",
    "train_error = (mis/X_train.shape[0])*100\n",
    "print('The training error, for minimizing C is obtained : \\n',train_error,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix for training is : \n",
      " [[284   3]\n",
      " [  6 161]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix_train = confusion_matrix(y_train,y_train_pred)\n",
    "\n",
    "print('The confusion matrix for training is : \\n',confusion_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix for train, appropriately indexed is : \n",
      "             Predicted 0  Predicted 1\n",
      "Actually 0          284            3\n",
      "Actually 1            6          161\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_train_df = pd.DataFrame(confusion_matrix_train,index=['Actually 0','Actually 1'],columns=['Predicted 0','Predicted 1'])\n",
    "print('The confusion matrix for train, appropriately indexed is : \\n',confusion_matrix_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for train is : \n",
      " 98.17073170731707 %\n",
      "The recall for train is : \n",
      " 96.40718562874252 %\n"
     ]
    }
   ],
   "source": [
    "precision_train = (confusion_matrix_train[1][1]/(confusion_matrix_train[1][1]+confusion_matrix_train[0][1]))*100\n",
    "recall_train = (confusion_matrix_train[1][1]/(confusion_matrix_train[1][1]+confusion_matrix_train[1][0]))*100\n",
    "\n",
    "print('The precision for train is : \\n',precision_train,'%')\n",
    "print('The recall for train is : \\n',recall_train,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for training is : \n",
      " 98.01762114537445 %\n",
      "The f1 score for training is : \n",
      " 0.9728096676737159\n"
     ]
    }
   ],
   "source": [
    "accuracy_train = ((confusion_matrix_train[0][0]+confusion_matrix_train[1][1])/(confusion_matrix_train[0][0]+confusion_matrix_train[1][1]+confusion_matrix_train[0][1]+confusion_matrix_train[1][0]))*100\n",
    "f1_train = ((2*(precision_train/100)*(recall_train/100))/((precision_train/100)+(recall_train/100))) # divided by 100 as precision and recall specified in percentage\n",
    "print('The accuracy for training is : \\n',accuracy_train,'%')\n",
    "print('The f1 score for training is : \\n',f1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix for test is : \n",
      " [[69  3]\n",
      " [ 0 43]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_test = confusion_matrix(y_test,y_test_pred)\n",
    "\n",
    "print('The confusion matrix for test is : \\n',confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix for test, appropriately indexed is : \n",
      "             Predicted 0  Predicted 1\n",
      "Actually 0           69            3\n",
      "Actually 1            0           43\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_test_df = pd.DataFrame(confusion_matrix_test,index=['Actually 0','Actually 1'],columns=['Predicted 0','Predicted 1'])\n",
    "print('The confusion matrix for test, appropriately indexed is : \\n',confusion_matrix_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for test is : \n",
      " 93.47826086956522 %\n",
      "The recall for test is : \n",
      " 100.0 %\n"
     ]
    }
   ],
   "source": [
    "precision_test = (confusion_matrix_test[1][1]/(confusion_matrix_test[1][1]+confusion_matrix_test[0][1]))*100\n",
    "recall_test = (confusion_matrix_test[1][1]/(confusion_matrix_test[1][1]+confusion_matrix_test[1][0]))*100\n",
    "\n",
    "print('The precision for test is : \\n',precision_test,'%')\n",
    "print('The recall for test is : \\n',recall_test,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for test is : \n",
      " 97.3913043478261 %\n",
      "The f1 score for test is : \n",
      " 0.9662921348314606\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = ((confusion_matrix_test[0][0]+confusion_matrix_test[1][1])/(confusion_matrix_test[0][0]+confusion_matrix_test[1][1]+confusion_matrix_test[0][1]+confusion_matrix_test[1][0]))*100\n",
    "f1_test = ((2*(precision_test/100)*(recall_test/100))/((precision_test/100)+(recall_test/100))) # divided by 100 as precision and recall specified in percentage\n",
    "print('The accuracy for test is : \\n',accuracy_test,'%')\n",
    "print('The f1 score for test is : \\n',f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.01695799e-01 7.98304201e-01]\n",
      " [9.92056127e-01 7.94387316e-03]\n",
      " [8.03594276e-01 1.96405724e-01]\n",
      " [1.80288664e-02 9.81971134e-01]\n",
      " [9.91774026e-01 8.22597365e-03]\n",
      " [5.06772549e-02 9.49322745e-01]\n",
      " [2.04688300e-01 7.95311700e-01]\n",
      " [9.96512045e-01 3.48795518e-03]\n",
      " [8.68394909e-01 1.31605091e-01]\n",
      " [5.94776777e-01 4.05223223e-01]\n",
      " [1.56507986e-04 9.99843492e-01]\n",
      " [6.58247572e-01 3.41752428e-01]\n",
      " [3.12150885e-01 6.87849115e-01]\n",
      " [2.34242127e-01 7.65757873e-01]\n",
      " [9.20121869e-01 7.98781311e-02]\n",
      " [9.99524253e-01 4.75746666e-04]\n",
      " [2.32739469e-01 7.67260531e-01]\n",
      " [3.12774853e-01 6.87225147e-01]\n",
      " [3.96833057e-02 9.60316694e-01]\n",
      " [9.73522971e-01 2.64770287e-02]\n",
      " [1.01306882e-02 9.89869312e-01]\n",
      " [9.73063063e-01 2.69369368e-02]\n",
      " [8.85757669e-01 1.14242331e-01]\n",
      " [9.01255743e-01 9.87442573e-02]\n",
      " [2.05122321e-02 9.79487768e-01]\n",
      " [9.43472377e-01 5.65276228e-02]\n",
      " [4.62099834e-04 9.99537900e-01]\n",
      " [9.19713155e-01 8.02868449e-02]\n",
      " [1.24720430e-02 9.87527957e-01]\n",
      " [2.36359753e-02 9.76364025e-01]\n",
      " [4.77406846e-02 9.52259315e-01]\n",
      " [6.74155665e-02 9.32584433e-01]\n",
      " [7.20433365e-01 2.79566635e-01]\n",
      " [9.54680766e-01 4.53192339e-02]\n",
      " [9.86756998e-01 1.32430017e-02]\n",
      " [7.23435952e-01 2.76564048e-01]\n",
      " [3.50295478e-02 9.64970452e-01]\n",
      " [9.78670990e-01 2.13290102e-02]\n",
      " [9.91453750e-01 8.54625017e-03]\n",
      " [9.95937997e-01 4.06200274e-03]\n",
      " [5.74845342e-02 9.42515466e-01]\n",
      " [9.44350985e-01 5.56490154e-02]\n",
      " [9.39889120e-01 6.01108799e-02]\n",
      " [9.67278406e-01 3.27215945e-02]\n",
      " [5.86730887e-02 9.41326911e-01]\n",
      " [9.46117051e-02 9.05388295e-01]\n",
      " [9.05457511e-01 9.45424889e-02]\n",
      " [9.82079593e-01 1.79204067e-02]\n",
      " [9.48984565e-01 5.10154349e-02]\n",
      " [9.99978816e-01 2.11838917e-05]\n",
      " [9.94432099e-01 5.56790056e-03]\n",
      " [6.09283217e-01 3.90716783e-01]\n",
      " [9.28594450e-01 7.14055497e-02]\n",
      " [2.25607804e-01 7.74392196e-01]\n",
      " [6.48604737e-01 3.51395263e-01]\n",
      " [9.64577064e-01 3.54229360e-02]\n",
      " [9.51426299e-01 4.85737007e-02]\n",
      " [9.98993702e-01 1.00629768e-03]\n",
      " [9.54286590e-01 4.57134102e-02]\n",
      " [6.50309956e-02 9.34969004e-01]\n",
      " [9.98892367e-01 1.10763284e-03]\n",
      " [9.93720769e-01 6.27923121e-03]\n",
      " [8.66726831e-01 1.33273169e-01]\n",
      " [9.74891466e-01 2.51085337e-02]\n",
      " [2.33554600e-03 9.97664454e-01]\n",
      " [2.79834189e-02 9.72016581e-01]\n",
      " [9.99718750e-01 2.81250357e-04]\n",
      " [7.55258511e-03 9.92447415e-01]\n",
      " [9.77985896e-01 2.20141043e-02]\n",
      " [3.04924566e-01 6.95075434e-01]\n",
      " [8.59429566e-01 1.40570434e-01]\n",
      " [8.44421014e-01 1.55578986e-01]\n",
      " [9.39211191e-01 6.07888087e-02]\n",
      " [9.92741864e-01 7.25813566e-03]\n",
      " [9.49070111e-01 5.09298889e-02]\n",
      " [6.33484575e-03 9.93665154e-01]\n",
      " [9.98433227e-01 1.56677310e-03]\n",
      " [9.97249620e-01 2.75038027e-03]\n",
      " [9.99589854e-01 4.10146333e-04]\n",
      " [2.71692160e-02 9.72830784e-01]\n",
      " [9.77658505e-01 2.23414949e-02]\n",
      " [9.23181012e-01 7.68189883e-02]\n",
      " [9.18339708e-01 8.16602917e-02]\n",
      " [9.81456964e-01 1.85430357e-02]\n",
      " [9.95026196e-01 4.97380412e-03]\n",
      " [9.79084673e-01 2.09153272e-02]\n",
      " [9.96392903e-01 3.60709741e-03]\n",
      " [9.53366093e-01 4.66339074e-02]\n",
      " [2.64198974e-02 9.73580103e-01]\n",
      " [6.09691279e-01 3.90308721e-01]\n",
      " [9.75459065e-01 2.45409347e-02]\n",
      " [9.57469206e-01 4.25307944e-02]\n",
      " [9.74072978e-01 2.59270217e-02]\n",
      " [5.51106442e-01 4.48893558e-01]\n",
      " [9.87000811e-01 1.29991895e-02]\n",
      " [9.96922390e-01 3.07761037e-03]\n",
      " [9.03988425e-01 9.60115749e-02]\n",
      " [9.67405587e-01 3.25944134e-02]\n",
      " [9.52201055e-01 4.77989453e-02]\n",
      " [9.66621561e-01 3.33784395e-02]\n",
      " [9.96464554e-01 3.53544619e-03]\n",
      " [9.94816722e-01 5.18327838e-03]\n",
      " [1.76359431e-01 8.23640569e-01]\n",
      " [8.48567001e-01 1.51432999e-01]\n",
      " [9.83220563e-01 1.67794367e-02]\n",
      " [1.17807123e-02 9.88219288e-01]\n",
      " [9.91407461e-01 8.59253891e-03]\n",
      " [2.67897057e-02 9.73210294e-01]\n",
      " [9.74380221e-01 2.56197788e-02]\n",
      " [1.33385776e-01 8.66614224e-01]\n",
      " [2.26241256e-01 7.73758744e-01]\n",
      " [9.99028649e-01 9.71351211e-04]\n",
      " [1.09088184e-02 9.89091182e-01]\n",
      " [9.98801503e-01 1.19849687e-03]\n",
      " [9.91338483e-01 8.66151687e-03]\n",
      " [9.74899154e-01 2.51008461e-02]\n",
      " [9.66025646e-01 3.39743539e-02]\n",
      " [2.23638453e-02 9.77636155e-01]\n",
      " [5.12514813e-03 9.94874852e-01]\n",
      " [9.62797059e-01 3.72029408e-02]\n",
      " [1.00368060e-02 9.89963194e-01]\n",
      " [8.26066529e-01 1.73933471e-01]\n",
      " [5.34667924e-01 4.65332076e-01]\n",
      " [2.18655255e-02 9.78134474e-01]\n",
      " [9.21503353e-01 7.84966470e-02]\n",
      " [9.54094692e-01 4.59053082e-02]\n",
      " [9.99216958e-01 7.83041537e-04]\n",
      " [8.13510137e-01 1.86489863e-01]\n",
      " [1.58356752e-02 9.84164325e-01]\n",
      " [9.11694709e-01 8.83052911e-02]\n",
      " [9.99901431e-01 9.85685134e-05]\n",
      " [5.03972859e-01 4.96027141e-01]\n",
      " [2.29246869e-01 7.70753131e-01]\n",
      " [9.96384832e-01 3.61516761e-03]\n",
      " [9.96649333e-01 3.35066704e-03]\n",
      " [2.97116250e-01 7.02883750e-01]\n",
      " [2.71736842e-02 9.72826316e-01]\n",
      " [1.62958703e-02 9.83704130e-01]\n",
      " [9.98768565e-01 1.23143499e-03]\n",
      " [9.97423026e-01 2.57697368e-03]\n",
      " [9.96629227e-01 3.37077258e-03]\n",
      " [8.03058784e-01 1.96941216e-01]\n",
      " [2.76873514e-01 7.23126486e-01]\n",
      " [3.94759161e-02 9.60524084e-01]\n",
      " [2.54111879e-03 9.97458881e-01]\n",
      " [1.24284714e-01 8.75715286e-01]\n",
      " [4.02435373e-01 5.97564627e-01]\n",
      " [6.11558502e-02 9.38844150e-01]\n",
      " [5.00926520e-01 4.99073480e-01]\n",
      " [6.24829104e-01 3.75170896e-01]\n",
      " [1.65996336e-01 8.34003664e-01]\n",
      " [2.01306498e-02 9.79869350e-01]\n",
      " [8.68330348e-01 1.31669652e-01]\n",
      " [2.38658181e-03 9.97613418e-01]\n",
      " [9.60488323e-01 3.95116770e-02]\n",
      " [2.70130621e-01 7.29869379e-01]\n",
      " [9.94155872e-01 5.84412840e-03]\n",
      " [2.00883484e-02 9.79911652e-01]\n",
      " [7.96063022e-03 9.92039370e-01]\n",
      " [8.83307456e-01 1.16692544e-01]\n",
      " [7.35481885e-01 2.64518115e-01]\n",
      " [9.99485129e-01 5.14871191e-04]\n",
      " [1.83884444e-01 8.16115556e-01]\n",
      " [1.55934102e-02 9.84406590e-01]\n",
      " [5.93270252e-03 9.94067297e-01]\n",
      " [3.73439265e-02 9.62656074e-01]\n",
      " [9.83365219e-01 1.66347814e-02]\n",
      " [9.99872072e-01 1.27927850e-04]\n",
      " [2.44307547e-01 7.55692453e-01]\n",
      " [7.35177411e-01 2.64822589e-01]\n",
      " [9.96546047e-01 3.45395320e-03]\n",
      " [9.34265417e-01 6.57345832e-02]\n",
      " [6.91301615e-01 3.08698385e-01]\n",
      " [7.19157070e-01 2.80842930e-01]\n",
      " [9.98402304e-01 1.59769572e-03]\n",
      " [2.38773736e-01 7.61226264e-01]\n",
      " [3.57792370e-01 6.42207630e-01]\n",
      " [9.61069550e-01 3.89304504e-02]\n",
      " [9.94404933e-01 5.59506660e-03]\n",
      " [7.58465238e-01 2.41534762e-01]\n",
      " [9.96848724e-01 3.15127606e-03]\n",
      " [3.17995260e-03 9.96820047e-01]\n",
      " [9.96515929e-01 3.48407145e-03]\n",
      " [8.33459936e-01 1.66540064e-01]\n",
      " [9.39794544e-01 6.02054562e-02]\n",
      " [3.61424115e-02 9.63857588e-01]\n",
      " [1.75903425e-01 8.24096575e-01]\n",
      " [6.72804065e-02 9.32719593e-01]\n",
      " [9.62108707e-01 3.78912934e-02]\n",
      " [6.51822821e-03 9.93481772e-01]\n",
      " [9.95171299e-01 4.82870110e-03]\n",
      " [9.93408907e-01 6.59109323e-03]\n",
      " [9.58090247e-01 4.19097531e-02]\n",
      " [9.98821467e-01 1.17853343e-03]\n",
      " [7.77710404e-02 9.22228960e-01]\n",
      " [9.82554993e-01 1.74450074e-02]\n",
      " [8.16303207e-01 1.83696793e-01]\n",
      " [9.72122463e-01 2.78775369e-02]\n",
      " [9.85102863e-01 1.48971368e-02]\n",
      " [9.76153508e-01 2.38464922e-02]\n",
      " [8.41502687e-01 1.58497313e-01]\n",
      " [9.67685871e-01 3.23141288e-02]\n",
      " [9.88022051e-01 1.19779486e-02]\n",
      " [9.88607753e-01 1.13922471e-02]\n",
      " [1.20720038e-01 8.79279962e-01]\n",
      " [3.48875955e-03 9.96511240e-01]\n",
      " [1.96216955e-02 9.80378304e-01]\n",
      " [8.73806151e-03 9.91261938e-01]\n",
      " [9.85876572e-01 1.41234276e-02]\n",
      " [7.75263381e-01 2.24736619e-01]\n",
      " [9.95365249e-01 4.63475052e-03]\n",
      " [7.96392395e-03 9.92036076e-01]\n",
      " [9.25880141e-01 7.41198588e-02]\n",
      " [9.71614357e-01 2.83856428e-02]\n",
      " [9.99798617e-01 2.01382588e-04]\n",
      " [9.82893411e-01 1.71065891e-02]\n",
      " [9.71079934e-01 2.89200663e-02]\n",
      " [9.75488131e-01 2.45118688e-02]\n",
      " [9.96391943e-01 3.60805738e-03]\n",
      " [1.71340529e-03 9.98286595e-01]\n",
      " [3.16574286e-02 9.68342571e-01]\n",
      " [2.38141622e-02 9.76185838e-01]\n",
      " [5.96206081e-03 9.94037939e-01]\n",
      " [9.89948942e-01 1.00510583e-02]\n",
      " [7.96829048e-01 2.03170952e-01]\n",
      " [9.94538799e-01 5.46120102e-03]\n",
      " [9.99953957e-01 4.60434820e-05]\n",
      " [9.99890317e-01 1.09682660e-04]\n",
      " [9.99903235e-01 9.67647324e-05]\n",
      " [9.99935781e-01 6.42185078e-05]\n",
      " [9.99844536e-01 1.55463811e-04]\n",
      " [9.99837011e-01 1.62989108e-04]\n",
      " [9.99768714e-01 2.31286388e-04]\n",
      " [9.99605885e-01 3.94115341e-04]\n",
      " [1.08971005e-03 9.98910290e-01]\n",
      " [9.99616668e-01 3.83331737e-04]\n",
      " [2.56987339e-03 9.97430127e-01]\n",
      " [9.99464273e-01 5.35727208e-04]\n",
      " [1.72122207e-03 9.98278778e-01]\n",
      " [9.99214778e-01 7.85222450e-04]\n",
      " [9.99226777e-01 7.73222539e-04]\n",
      " [9.99394691e-01 6.05309094e-04]\n",
      " [9.99374699e-01 6.25301259e-04]\n",
      " [9.99277061e-01 7.22938701e-04]\n",
      " [9.99134875e-01 8.65125254e-04]\n",
      " [9.99087210e-01 9.12790256e-04]\n",
      " [9.98881225e-01 1.11877529e-03]\n",
      " [3.81508253e-03 9.96184917e-01]\n",
      " [2.83638183e-03 9.97163618e-01]\n",
      " [9.98625846e-01 1.37415354e-03]\n",
      " [3.18262148e-03 9.96817379e-01]\n",
      " [9.97743788e-01 2.25621194e-03]\n",
      " [4.45675101e-03 9.95543249e-01]\n",
      " [9.98090000e-01 1.91000046e-03]\n",
      " [9.97624817e-01 2.37518263e-03]\n",
      " [4.97993127e-03 9.95020069e-01]\n",
      " [5.10117316e-03 9.94898827e-01]\n",
      " [6.84113754e-03 9.93158862e-01]\n",
      " [7.40456890e-03 9.92595431e-01]\n",
      " [9.96516357e-01 3.48364316e-03]\n",
      " [7.31957821e-03 9.92680422e-01]\n",
      " [7.64109099e-03 9.92358909e-01]\n",
      " [6.40541325e-03 9.93594587e-01]\n",
      " [7.67917964e-03 9.92320820e-01]\n",
      " [9.95170879e-01 4.82912091e-03]\n",
      " [9.09708481e-03 9.90902915e-01]\n",
      " [9.96461375e-01 3.53862541e-03]\n",
      " [9.95474449e-01 4.52555063e-03]\n",
      " [9.95107390e-01 4.89260972e-03]\n",
      " [9.95167287e-01 4.83271324e-03]\n",
      " [8.67975337e-03 9.91320247e-01]\n",
      " [9.94904914e-01 5.09508565e-03]\n",
      " [9.95064017e-01 4.93598338e-03]\n",
      " [9.95064815e-01 4.93518511e-03]\n",
      " [9.93933243e-01 6.06675693e-03]\n",
      " [9.92936215e-01 7.06378499e-03]\n",
      " [9.95141776e-01 4.85822425e-03]\n",
      " [8.61506176e-03 9.91384938e-01]\n",
      " [9.93841826e-01 6.15817435e-03]\n",
      " [9.90720167e-01 9.27983324e-03]\n",
      " [9.91647892e-01 8.35210818e-03]\n",
      " [9.92650998e-01 7.34900183e-03]\n",
      " [9.87701835e-01 1.22981652e-02]\n",
      " [9.92146074e-01 7.85392628e-03]\n",
      " [1.21574352e-02 9.87842565e-01]\n",
      " [1.47376678e-02 9.85262332e-01]\n",
      " [9.90892152e-01 9.10784790e-03]\n",
      " [9.89925652e-01 1.00743484e-02]\n",
      " [9.90213980e-01 9.78601958e-03]\n",
      " [9.90164825e-01 9.83517477e-03]\n",
      " [1.59813114e-02 9.84018689e-01]\n",
      " [9.91390645e-01 8.60935465e-03]\n",
      " [9.89775014e-01 1.02249859e-02]\n",
      " [1.43601198e-02 9.85639880e-01]\n",
      " [9.90808228e-01 9.19177220e-03]\n",
      " [9.90489938e-01 9.51006150e-03]\n",
      " [2.26901829e-02 9.77309817e-01]\n",
      " [9.89505478e-01 1.04945216e-02]\n",
      " [9.89121387e-01 1.08786131e-02]\n",
      " [1.85613465e-02 9.81438653e-01]\n",
      " [3.36729202e-02 9.66327080e-01]\n",
      " [2.11021259e-02 9.78897874e-01]\n",
      " [9.89034021e-01 1.09659794e-02]\n",
      " [9.87392950e-01 1.26070496e-02]\n",
      " [9.86976826e-01 1.30231738e-02]\n",
      " [9.86769501e-01 1.32304987e-02]\n",
      " [1.74132454e-02 9.82586755e-01]\n",
      " [2.13158247e-02 9.78684175e-01]\n",
      " [3.11277709e-02 9.68872229e-01]\n",
      " [9.81288465e-01 1.87115350e-02]\n",
      " [9.81483269e-01 1.85167310e-02]\n",
      " [9.83708652e-01 1.62913477e-02]\n",
      " [2.86791632e-02 9.71320837e-01]\n",
      " [9.80856824e-01 1.91431763e-02]\n",
      " [2.96191194e-02 9.70380881e-01]\n",
      " [9.83788098e-01 1.62119024e-02]\n",
      " [9.78302242e-01 2.16977580e-02]\n",
      " [9.80599028e-01 1.94009717e-02]\n",
      " [3.64844267e-02 9.63515573e-01]\n",
      " [9.78801078e-01 2.11989215e-02]\n",
      " [9.69933169e-01 3.00668314e-02]\n",
      " [4.55177319e-02 9.54482268e-01]\n",
      " [9.76898983e-01 2.31010168e-02]\n",
      " [9.76771896e-01 2.32281044e-02]\n",
      " [9.80734846e-01 1.92651545e-02]\n",
      " [6.06626406e-02 9.39337359e-01]\n",
      " [9.53175099e-01 4.68249007e-02]\n",
      " [5.39899421e-02 9.46010058e-01]\n",
      " [4.72939784e-02 9.52706022e-01]\n",
      " [9.65617445e-01 3.43825551e-02]\n",
      " [4.82691819e-02 9.51730818e-01]\n",
      " [3.52557339e-02 9.64744266e-01]\n",
      " [4.13352531e-02 9.58664747e-01]\n",
      " [9.66909684e-01 3.30903157e-02]\n",
      " [4.62458251e-02 9.53754175e-01]\n",
      " [9.69325723e-01 3.06742767e-02]\n",
      " [4.38693985e-02 9.56130602e-01]\n",
      " [4.99573262e-02 9.50042674e-01]\n",
      " [9.61472188e-01 3.85278118e-02]\n",
      " [9.66041431e-01 3.39585692e-02]\n",
      " [9.65678700e-01 3.43213004e-02]\n",
      " [9.67455435e-01 3.25445653e-02]\n",
      " [6.12511267e-02 9.38748873e-01]\n",
      " [9.66692637e-01 3.33073627e-02]\n",
      " [9.59787905e-01 4.02120949e-02]\n",
      " [9.56045290e-01 4.39547099e-02]\n",
      " [9.57578472e-01 4.24215280e-02]\n",
      " [9.53909647e-01 4.60903534e-02]\n",
      " [9.65146754e-01 3.48532460e-02]\n",
      " [9.57058464e-01 4.29415363e-02]\n",
      " [9.59953090e-01 4.00469101e-02]\n",
      " [5.25570686e-02 9.47442931e-01]\n",
      " [9.49075477e-01 5.09245228e-02]\n",
      " [1.07736615e-01 8.92263385e-01]\n",
      " [6.24546745e-02 9.37545325e-01]\n",
      " [7.24866733e-02 9.27513327e-01]\n",
      " [8.62262146e-02 9.13773785e-01]\n",
      " [9.52962014e-01 4.70379859e-02]\n",
      " [9.52728329e-01 4.72716709e-02]\n",
      " [9.39527921e-01 6.04720786e-02]\n",
      " [9.32356206e-01 6.76437941e-02]\n",
      " [7.38168528e-02 9.26183147e-01]\n",
      " [9.41235906e-01 5.87640939e-02]\n",
      " [9.78710921e-02 9.02128908e-01]\n",
      " [9.36251579e-01 6.37484207e-02]\n",
      " [1.05586260e-01 8.94413740e-01]\n",
      " [1.03181819e-01 8.96818181e-01]\n",
      " [9.42914783e-01 5.70852174e-02]\n",
      " [9.34936363e-01 6.50636372e-02]\n",
      " [9.37053596e-01 6.29464035e-02]\n",
      " [9.43223676e-02 9.05677632e-01]\n",
      " [9.26694675e-02 9.07330532e-01]\n",
      " [9.32588393e-01 6.74116065e-02]\n",
      " [9.19209709e-01 8.07902907e-02]\n",
      " [9.34880235e-01 6.51197652e-02]\n",
      " [9.18213903e-01 8.17860971e-02]\n",
      " [9.31646517e-01 6.83534828e-02]\n",
      " [9.26309567e-01 7.36904330e-02]\n",
      " [1.14925493e-01 8.85074507e-01]\n",
      " [9.11832830e-01 8.81671702e-02]\n",
      " [9.18278610e-01 8.17213905e-02]\n",
      " [9.14731025e-01 8.52689747e-02]\n",
      " [1.07681179e-01 8.92318821e-01]\n",
      " [9.32464056e-01 6.75359438e-02]\n",
      " [1.11940779e-01 8.88059221e-01]\n",
      " [1.56806977e-01 8.43193023e-01]\n",
      " [9.13085216e-01 8.69147844e-02]\n",
      " [1.10697403e-01 8.89302597e-01]\n",
      " [1.49090879e-01 8.50909121e-01]\n",
      " [1.47431373e-01 8.52568627e-01]\n",
      " [1.54527931e-01 8.45472069e-01]\n",
      " [8.81032855e-01 1.18967145e-01]\n",
      " [1.64141099e-01 8.35858901e-01]\n",
      " [8.86187080e-01 1.13812920e-01]\n",
      " [8.99258995e-01 1.00741005e-01]\n",
      " [1.66491273e-01 8.33508727e-01]\n",
      " [8.70391898e-01 1.29608102e-01]\n",
      " [1.50154669e-01 8.49845331e-01]\n",
      " [8.66852496e-01 1.33147504e-01]\n",
      " [8.69052492e-01 1.30947508e-01]\n",
      " [8.76436197e-01 1.23563803e-01]\n",
      " [8.73186914e-01 1.26813086e-01]\n",
      " [8.53107977e-01 1.46892023e-01]\n",
      " [8.70692641e-01 1.29307359e-01]\n",
      " [8.62959364e-01 1.37040636e-01]\n",
      " [8.71363043e-01 1.28636957e-01]\n",
      " [8.79355729e-01 1.20644271e-01]\n",
      " [8.70990427e-01 1.29009573e-01]\n",
      " [8.69450404e-01 1.30549596e-01]\n",
      " [2.15495979e-01 7.84504021e-01]\n",
      " [8.32498079e-01 1.67501921e-01]\n",
      " [8.19451819e-01 1.80548181e-01]\n",
      " [8.31991720e-01 1.68008280e-01]\n",
      " [8.40874138e-01 1.59125862e-01]\n",
      " [8.28735369e-01 1.71264631e-01]\n",
      " [2.30578770e-01 7.69421230e-01]\n",
      " [8.34171505e-01 1.65828495e-01]\n",
      " [8.16748310e-01 1.83251690e-01]\n",
      " [8.26621470e-01 1.73378530e-01]\n",
      " [8.12106353e-01 1.87893647e-01]\n",
      " [8.20530770e-01 1.79469230e-01]\n",
      " [2.34760116e-01 7.65239884e-01]\n",
      " [2.83975691e-01 7.16024309e-01]\n",
      " [7.90445234e-01 2.09554766e-01]\n",
      " [3.07298447e-01 6.92701553e-01]\n",
      " [7.72648835e-01 2.27351165e-01]\n",
      " [7.46612211e-01 2.53387789e-01]\n",
      " [3.49668590e-01 6.50331410e-01]\n",
      " [7.90033307e-01 2.09966693e-01]\n",
      " [3.26775678e-01 6.73224322e-01]\n",
      " [8.14667966e-01 1.85332034e-01]\n",
      " [8.33500412e-01 1.66499588e-01]\n",
      " [2.70117967e-01 7.29882033e-01]\n",
      " [7.67633972e-01 2.32366028e-01]\n",
      " [7.62172301e-01 2.37827699e-01]\n",
      " [7.53918002e-01 2.46081998e-01]\n",
      " [3.02768097e-01 6.97231903e-01]\n",
      " [7.43253283e-01 2.56746717e-01]\n",
      " [3.80518379e-01 6.19481621e-01]\n",
      " [3.81773017e-01 6.18226983e-01]\n",
      " [3.33648545e-01 6.66351455e-01]\n",
      " [3.91080121e-01 6.08919879e-01]\n",
      " [6.59376758e-01 3.40623242e-01]\n",
      " [6.39246417e-01 3.60753583e-01]\n",
      " [6.39107889e-01 3.60892111e-01]\n",
      " [6.12889675e-01 3.87110325e-01]\n",
      " [4.23357999e-01 5.76642001e-01]\n",
      " [4.04307527e-01 5.95692473e-01]\n",
      " [3.76988036e-01 6.23011964e-01]\n",
      " [4.70984806e-01 5.29015194e-01]\n",
      " [5.64200978e-01 4.35799022e-01]\n",
      " [4.07561121e-01 5.92438879e-01]\n",
      " [6.43879217e-01 3.56120783e-01]\n",
      " [5.45882209e-01 4.54117791e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "clf_prob = LinearSVC(penalty='l1',dual=False,C=min_C)\n",
    "calibrated_svc = CalibratedClassifierCV(clf_prob,method='sigmoid')\n",
    "calibrated_svc.fit(X_train,y_train)\n",
    "y_train_pred_prob = calibrated_svc.predict_proba(X_train)\n",
    "\n",
    "print(y_train_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.01695799e-01 9.92056127e-01 8.03594276e-01 1.80288664e-02\n",
      " 9.91774026e-01 5.06772549e-02 2.04688300e-01 9.96512045e-01\n",
      " 8.68394909e-01 5.94776777e-01 1.56507986e-04 6.58247572e-01\n",
      " 3.12150885e-01 2.34242127e-01 9.20121869e-01 9.99524253e-01\n",
      " 2.32739469e-01 3.12774853e-01 3.96833057e-02 9.73522971e-01\n",
      " 1.01306882e-02 9.73063063e-01 8.85757669e-01 9.01255743e-01\n",
      " 2.05122321e-02 9.43472377e-01 4.62099834e-04 9.19713155e-01\n",
      " 1.24720430e-02 2.36359753e-02 4.77406846e-02 6.74155665e-02\n",
      " 7.20433365e-01 9.54680766e-01 9.86756998e-01 7.23435952e-01\n",
      " 3.50295478e-02 9.78670990e-01 9.91453750e-01 9.95937997e-01\n",
      " 5.74845342e-02 9.44350985e-01 9.39889120e-01 9.67278406e-01\n",
      " 5.86730887e-02 9.46117051e-02 9.05457511e-01 9.82079593e-01\n",
      " 9.48984565e-01 9.99978816e-01 9.94432099e-01 6.09283217e-01\n",
      " 9.28594450e-01 2.25607804e-01 6.48604737e-01 9.64577064e-01\n",
      " 9.51426299e-01 9.98993702e-01 9.54286590e-01 6.50309956e-02\n",
      " 9.98892367e-01 9.93720769e-01 8.66726831e-01 9.74891466e-01\n",
      " 2.33554600e-03 2.79834189e-02 9.99718750e-01 7.55258511e-03\n",
      " 9.77985896e-01 3.04924566e-01 8.59429566e-01 8.44421014e-01\n",
      " 9.39211191e-01 9.92741864e-01 9.49070111e-01 6.33484575e-03\n",
      " 9.98433227e-01 9.97249620e-01 9.99589854e-01 2.71692160e-02\n",
      " 9.77658505e-01 9.23181012e-01 9.18339708e-01 9.81456964e-01\n",
      " 9.95026196e-01 9.79084673e-01 9.96392903e-01 9.53366093e-01\n",
      " 2.64198974e-02 6.09691279e-01 9.75459065e-01 9.57469206e-01\n",
      " 9.74072978e-01 5.51106442e-01 9.87000811e-01 9.96922390e-01\n",
      " 9.03988425e-01 9.67405587e-01 9.52201055e-01 9.66621561e-01\n",
      " 9.96464554e-01 9.94816722e-01 1.76359431e-01 8.48567001e-01\n",
      " 9.83220563e-01 1.17807123e-02 9.91407461e-01 2.67897057e-02\n",
      " 9.74380221e-01 1.33385776e-01 2.26241256e-01 9.99028649e-01\n",
      " 1.09088184e-02 9.98801503e-01 9.91338483e-01 9.74899154e-01\n",
      " 9.66025646e-01 2.23638453e-02 5.12514813e-03 9.62797059e-01\n",
      " 1.00368060e-02 8.26066529e-01 5.34667924e-01 2.18655255e-02\n",
      " 9.21503353e-01 9.54094692e-01 9.99216958e-01 8.13510137e-01\n",
      " 1.58356752e-02 9.11694709e-01 9.99901431e-01 5.03972859e-01\n",
      " 2.29246869e-01 9.96384832e-01 9.96649333e-01 2.97116250e-01\n",
      " 2.71736842e-02 1.62958703e-02 9.98768565e-01 9.97423026e-01\n",
      " 9.96629227e-01 8.03058784e-01 2.76873514e-01 3.94759161e-02\n",
      " 2.54111879e-03 1.24284714e-01 4.02435373e-01 6.11558502e-02\n",
      " 5.00926520e-01 6.24829104e-01 1.65996336e-01 2.01306498e-02\n",
      " 8.68330348e-01 2.38658181e-03 9.60488323e-01 2.70130621e-01\n",
      " 9.94155872e-01 2.00883484e-02 7.96063022e-03 8.83307456e-01\n",
      " 7.35481885e-01 9.99485129e-01 1.83884444e-01 1.55934102e-02\n",
      " 5.93270252e-03 3.73439265e-02 9.83365219e-01 9.99872072e-01\n",
      " 2.44307547e-01 7.35177411e-01 9.96546047e-01 9.34265417e-01\n",
      " 6.91301615e-01 7.19157070e-01 9.98402304e-01 2.38773736e-01\n",
      " 3.57792370e-01 9.61069550e-01 9.94404933e-01 7.58465238e-01\n",
      " 9.96848724e-01 3.17995260e-03 9.96515929e-01 8.33459936e-01\n",
      " 9.39794544e-01 3.61424115e-02 1.75903425e-01 6.72804065e-02\n",
      " 9.62108707e-01 6.51822821e-03 9.95171299e-01 9.93408907e-01\n",
      " 9.58090247e-01 9.98821467e-01 7.77710404e-02 9.82554993e-01\n",
      " 8.16303207e-01 9.72122463e-01 9.85102863e-01 9.76153508e-01\n",
      " 8.41502687e-01 9.67685871e-01 9.88022051e-01 9.88607753e-01\n",
      " 1.20720038e-01 3.48875955e-03 1.96216955e-02 8.73806151e-03\n",
      " 9.85876572e-01 7.75263381e-01 9.95365249e-01 7.96392395e-03\n",
      " 9.25880141e-01 9.71614357e-01 9.99798617e-01 9.82893411e-01\n",
      " 9.71079934e-01 9.75488131e-01 9.96391943e-01 1.71340529e-03\n",
      " 3.16574286e-02 2.38141622e-02 5.96206081e-03 9.89948942e-01\n",
      " 7.96829048e-01 9.94538799e-01 9.99953957e-01 9.99890317e-01\n",
      " 9.99903235e-01 9.99935781e-01 9.99844536e-01 9.99837011e-01\n",
      " 9.99768714e-01 9.99605885e-01 1.08971005e-03 9.99616668e-01\n",
      " 2.56987339e-03 9.99464273e-01 1.72122207e-03 9.99214778e-01\n",
      " 9.99226777e-01 9.99394691e-01 9.99374699e-01 9.99277061e-01\n",
      " 9.99134875e-01 9.99087210e-01 9.98881225e-01 3.81508253e-03\n",
      " 2.83638183e-03 9.98625846e-01 3.18262148e-03 9.97743788e-01\n",
      " 4.45675101e-03 9.98090000e-01 9.97624817e-01 4.97993127e-03\n",
      " 5.10117316e-03 6.84113754e-03 7.40456890e-03 9.96516357e-01\n",
      " 7.31957821e-03 7.64109099e-03 6.40541325e-03 7.67917964e-03\n",
      " 9.95170879e-01 9.09708481e-03 9.96461375e-01 9.95474449e-01\n",
      " 9.95107390e-01 9.95167287e-01 8.67975337e-03 9.94904914e-01\n",
      " 9.95064017e-01 9.95064815e-01 9.93933243e-01 9.92936215e-01\n",
      " 9.95141776e-01 8.61506176e-03 9.93841826e-01 9.90720167e-01\n",
      " 9.91647892e-01 9.92650998e-01 9.87701835e-01 9.92146074e-01\n",
      " 1.21574352e-02 1.47376678e-02 9.90892152e-01 9.89925652e-01\n",
      " 9.90213980e-01 9.90164825e-01 1.59813114e-02 9.91390645e-01\n",
      " 9.89775014e-01 1.43601198e-02 9.90808228e-01 9.90489938e-01\n",
      " 2.26901829e-02 9.89505478e-01 9.89121387e-01 1.85613465e-02\n",
      " 3.36729202e-02 2.11021259e-02 9.89034021e-01 9.87392950e-01\n",
      " 9.86976826e-01 9.86769501e-01 1.74132454e-02 2.13158247e-02\n",
      " 3.11277709e-02 9.81288465e-01 9.81483269e-01 9.83708652e-01\n",
      " 2.86791632e-02 9.80856824e-01 2.96191194e-02 9.83788098e-01\n",
      " 9.78302242e-01 9.80599028e-01 3.64844267e-02 9.78801078e-01\n",
      " 9.69933169e-01 4.55177319e-02 9.76898983e-01 9.76771896e-01\n",
      " 9.80734846e-01 6.06626406e-02 9.53175099e-01 5.39899421e-02\n",
      " 4.72939784e-02 9.65617445e-01 4.82691819e-02 3.52557339e-02\n",
      " 4.13352531e-02 9.66909684e-01 4.62458251e-02 9.69325723e-01\n",
      " 4.38693985e-02 4.99573262e-02 9.61472188e-01 9.66041431e-01\n",
      " 9.65678700e-01 9.67455435e-01 6.12511267e-02 9.66692637e-01\n",
      " 9.59787905e-01 9.56045290e-01 9.57578472e-01 9.53909647e-01\n",
      " 9.65146754e-01 9.57058464e-01 9.59953090e-01 5.25570686e-02\n",
      " 9.49075477e-01 1.07736615e-01 6.24546745e-02 7.24866733e-02\n",
      " 8.62262146e-02 9.52962014e-01 9.52728329e-01 9.39527921e-01\n",
      " 9.32356206e-01 7.38168528e-02 9.41235906e-01 9.78710921e-02\n",
      " 9.36251579e-01 1.05586260e-01 1.03181819e-01 9.42914783e-01\n",
      " 9.34936363e-01 9.37053596e-01 9.43223676e-02 9.26694675e-02\n",
      " 9.32588393e-01 9.19209709e-01 9.34880235e-01 9.18213903e-01\n",
      " 9.31646517e-01 9.26309567e-01 1.14925493e-01 9.11832830e-01\n",
      " 9.18278610e-01 9.14731025e-01 1.07681179e-01 9.32464056e-01\n",
      " 1.11940779e-01 1.56806977e-01 9.13085216e-01 1.10697403e-01\n",
      " 1.49090879e-01 1.47431373e-01 1.54527931e-01 8.81032855e-01\n",
      " 1.64141099e-01 8.86187080e-01 8.99258995e-01 1.66491273e-01\n",
      " 8.70391898e-01 1.50154669e-01 8.66852496e-01 8.69052492e-01\n",
      " 8.76436197e-01 8.73186914e-01 8.53107977e-01 8.70692641e-01\n",
      " 8.62959364e-01 8.71363043e-01 8.79355729e-01 8.70990427e-01\n",
      " 8.69450404e-01 2.15495979e-01 8.32498079e-01 8.19451819e-01\n",
      " 8.31991720e-01 8.40874138e-01 8.28735369e-01 2.30578770e-01\n",
      " 8.34171505e-01 8.16748310e-01 8.26621470e-01 8.12106353e-01\n",
      " 8.20530770e-01 2.34760116e-01 2.83975691e-01 7.90445234e-01\n",
      " 3.07298447e-01 7.72648835e-01 7.46612211e-01 3.49668590e-01\n",
      " 7.90033307e-01 3.26775678e-01 8.14667966e-01 8.33500412e-01\n",
      " 2.70117967e-01 7.67633972e-01 7.62172301e-01 7.53918002e-01\n",
      " 3.02768097e-01 7.43253283e-01 3.80518379e-01 3.81773017e-01\n",
      " 3.33648545e-01 3.91080121e-01 6.59376758e-01 6.39246417e-01\n",
      " 6.39107889e-01 6.12889675e-01 4.23357999e-01 4.04307527e-01\n",
      " 3.76988036e-01 4.70984806e-01 5.64200978e-01 4.07561121e-01\n",
      " 6.43879217e-01 5.45882209e-01]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred_prob_0 = y_train_pred_prob[:,0]\n",
    "print(y_train_pred_prob_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           FPR       TPR\n",
      "0.00         0         0\n",
      "0.01         0  0.209581\n",
      "0.02         0  0.305389\n",
      "0.03         0  0.407186\n",
      "0.04         0  0.467066\n",
      "...        ...       ...\n",
      "0.96  0.439024         1\n",
      "0.97  0.508711         1\n",
      "0.98  0.578397         1\n",
      "0.99  0.679443         1\n",
      "1.00         1         1\n",
      "\n",
      "[101 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "thresh = np.arange(0,1.01,0.01)\n",
    "ROC_df = pd.DataFrame(index=thresh,columns=['FPR','TPR'])\n",
    "for i in np.arange(0,thresh.size):\n",
    "    y_train_pred_roc = (y_train_pred_prob_0 <= thresh[i]).astype(int)\n",
    "    confusion_matrix_train_roc = confusion_matrix(y_train,y_train_pred_roc)\n",
    "    ROC_df.iloc[i,0] = confusion_matrix_train_roc[0][1]/(confusion_matrix_train_roc[0][1]+confusion_matrix_train_roc[0][0])\n",
    "    ROC_df.iloc[i,1] = confusion_matrix_train_roc[1][1]/(confusion_matrix_train_roc[1][1]+confusion_matrix_train_roc[1][0])\n",
    "print(ROC_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'TPR for Train')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(ROC_df['FPR'],ROC_df['TPR'])\n",
    "plt.xlabel('FPR for Train')\n",
    "plt.ylabel('TPR for Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for train is : \n",
      " 0.997005988023952\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "AUC_train = metrics.auc(ROC_df['FPR'],ROC_df['TPR'])\n",
    "print('The AUC for train is : \\n',AUC_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.41763492e-01 2.92147094e-02 9.65777949e-01 9.19113749e-01\n",
      " 9.00401923e-01 9.97063160e-01 1.52757764e-02 2.99885905e-02\n",
      " 2.16733331e-02 3.15224336e-02 9.50798174e-01 6.11059981e-03\n",
      " 9.86508482e-01 5.12720517e-03 9.94282283e-01 9.99763437e-01\n",
      " 5.67359182e-01 2.43914534e-01 9.95788531e-01 6.25014918e-02\n",
      " 5.69796045e-03 2.55416633e-03 8.34022548e-01 7.48865134e-01\n",
      " 9.84994149e-01 9.85605604e-01 9.95998692e-01 8.76181641e-01\n",
      " 3.31641516e-02 9.30407618e-01 9.61635839e-01 9.65021806e-01\n",
      " 9.03174066e-01 7.15132952e-02 3.33784341e-02 9.74703701e-01\n",
      " 4.37881465e-02 9.66193576e-01 8.74965342e-01 3.13033709e-01\n",
      " 1.32766005e-01 9.68754220e-01 9.71146146e-01 5.13261824e-01\n",
      " 7.66035550e-01 6.18726694e-02 6.57237861e-02 7.57619951e-01\n",
      " 2.99044568e-01 9.83756264e-01 8.41096957e-02 9.95997781e-01\n",
      " 9.94314752e-01 8.62874014e-01 9.56887434e-02 8.74621724e-01\n",
      " 9.96548188e-01 1.78585391e-03 1.28632474e-02 9.87371461e-01\n",
      " 6.89356160e-01 1.21049725e-01 9.99982563e-01 1.24517752e-02\n",
      " 4.43293952e-01 9.88557277e-01 9.56440460e-01 1.94779646e-01\n",
      " 9.97368516e-01 5.55798151e-01 2.71105892e-01 9.70273530e-01\n",
      " 8.83678401e-01 9.91213166e-01 6.79604443e-02 2.72939848e-02\n",
      " 1.57043745e-01 1.00428991e-02 7.64732516e-01 1.94375919e-02\n",
      " 9.51096600e-01 6.85863396e-03 9.99316958e-01 9.50239312e-01\n",
      " 9.96956700e-01 9.99149126e-01 1.94807324e-02 9.99920110e-01\n",
      " 9.93952011e-01 9.91388188e-01 6.25225150e-01 9.76998484e-01\n",
      " 6.27695726e-02 5.60094777e-04 2.28212235e-02 8.01386618e-01\n",
      " 9.39672890e-01 8.83624814e-01 9.98809236e-01 9.96650633e-01\n",
      " 1.44072293e-02 1.01584726e-01 9.39055819e-01 2.98288846e-01\n",
      " 9.60446355e-02 7.98162796e-01 9.99475240e-01 9.39382583e-01\n",
      " 9.90416668e-01 9.69188663e-01 9.80752822e-01 9.78015986e-01\n",
      " 9.45500899e-01 8.42027976e-01 7.40997261e-03]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_prob = calibrated_svc.predict_proba(X_test)\n",
    "y_test_pred_prob_0 = y_test_pred_prob[:,0]\n",
    "print(y_test_pred_prob_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           FPR       TPR\n",
      "0.00         0         0\n",
      "0.01         0  0.186047\n",
      "0.02         0  0.348837\n",
      "0.03         0  0.465116\n",
      "0.04         0  0.534884\n",
      "...        ...       ...\n",
      "0.96  0.458333         1\n",
      "0.97  0.541667         1\n",
      "0.98  0.611111         1\n",
      "0.99  0.708333         1\n",
      "1.00         1         1\n",
      "\n",
      "[101 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "thresh = np.arange(0,1.01,0.01)\n",
    "ROC_df_test = pd.DataFrame(index=thresh,columns=['FPR','TPR'])\n",
    "for i in np.arange(0,thresh.size):\n",
    "    y_test_pred_roc = (y_test_pred_prob_0 <= thresh[i]).astype(int)\n",
    "    confusion_matrix_test_roc = confusion_matrix(y_test,y_test_pred_roc)\n",
    "    ROC_df_test.iloc[i,0] = confusion_matrix_test_roc[0][1]/(confusion_matrix_test_roc[0][1]+confusion_matrix_test_roc[0][0])\n",
    "    ROC_df_test.iloc[i,1] = confusion_matrix_test_roc[1][1]/(confusion_matrix_test_roc[1][1]+confusion_matrix_test_roc[1][0])\n",
    "print(ROC_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'TPR for Test')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAUDklEQVR4nO3df7DldX3f8efL3SAqPzTu2hB+LcQl40oSsHeoacaEDMZZiLJphzrQ4g+k0tqgNVinZGLB4LQ1moTWKSnZREZDRgGtiTt2DWYSxNQK4TIgYdeSuVkUVmhZjcVfUcS8+8f5Ek/Pnnv23L33e673fp6PmTOc7/f7ud/v+8O9e17n8/2ZqkKS1K6nrXYBkqTVZRBIUuMMAklqnEEgSY0zCCSpcRtXu4Cl2rRpU23ZsmW1y5CkNeXuu+/+UlVtHrdszQXBli1bmJ+fX+0yJGlNSfKFxZa5a0iSGmcQSFLjDAJJapxBIEmNMwgkqXG9BUGSG5I8luT+RZYnyXuSLCS5L8mL+qpFkrS4PkcE7wO2T1h+LrC1e10G/Ncea5EkLaK36wiq6lNJtkxosgP4vRrcB/uOJM9OclxVPdpXTcv1gTsf4qP3fnG1y5DUqG0/fAxXv+KFK77e1TxGcDzw8ND0/m7eQZJclmQ+yfyBAwdmUtw4H733i+x99Kurtn1J6sNqXlmcMfPGPiWnqnYCOwHm5uZW9Uk62447hpv/xU+uZgmStKJWc0SwHzhxaPoE4JFVqkWSmrWaQbALeHV39tCLgce/n48PSNJ61duuoSQfBM4GNiXZD1wN/ABAVV0P7AbOAxaAbwKX9FXL4Ro9OLz30a+y7bhjVrEiSVp5fZ41dNEhlhfwi31tfyU8dXD4qQ//bccdw44zxh7PlqQ1a83dhnrWPDgsab3zFhOS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcZ4+OsQLyCS1yBHBkNG7i3oBmaQWOCIY4QVkklrjiECSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1rpnTR0cvFhvHC8gktaiZEcHoxWLjeAGZpBY1MyIALxaTpHGaGRFIksYzCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMb1GgRJtid5IMlCkivHLD8pyW1J7klyX5Lz+qxHknSw3oIgyQbgOuBcYBtwUZJtI83eBtxSVWcCFwK/1Vc9kqTx+hwRnAUsVNW+qnoCuAnYMdKmgKeeBHMs8EiP9UiSxugzCI4HHh6a3t/NG/Z24OIk+4HdwBvHrSjJZUnmk8wfOHCgj1olqVl9BkHGzKuR6YuA91XVCcB5wI1JDqqpqnZW1VxVzW3evLmHUiWpXX0GwX7gxKHpEzh418+lwC0AVfUZ4EhgU481SZJG9BkEdwFbk5yS5AgGB4N3jbR5CDgHIMkLGASB+34kaYZ6C4KqehK4HLgV+ByDs4P2JLkmyflds7cAr0/yWeCDwGuranT3kSSpR70+vL6qdjM4CDw876qh93uBn+qzBknSZF5ZLEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNO2QQJPnENPMkSWvTxsUWJDkCOBL4e0mOBtItOgY4aQa1SZJmYNEgAH4RuAJ4HrCH7wXBV4Hre65LkjQjiwZBVV0LXJvkzVX1n2ZYkyRphqY5WPxQt2uIJFcmuSXJGT3XJUmakWmC4O1V9bUk/xB4BXAzU+4aSrI9yQNJFpJcuUibVybZm2RPkg9MX7okaSVMEwTf7f77cuC3quq/AU8/1A8l2QBcB5wLbAMuSrJtpM1W4JeBn6qqFwJvXkLtkqQVMOlg8VMeTXIdsB2Y684mmiZAzgIWqmofQJKbgB3A3qE2rweuq6qvAFTVY0spXpK0fNN8oL8SuB34+e4DexMwdjfPiOOBh4em93fzhp0GnJbk00nuSLJ93IqSXJZkPsn8gQMHpti0JGlahwyCqvo68BCDb/gA32ZwOumhZMy8GpneCGwFzgYuAn43ybPH1LCzquaqam7z5s1TbFqSNK1prix+G3A18LZu1pHANAd19wMnDk2fADwyps1Hq+o7VfUg8ACDYJAkzcg0u4YuAM4DvgFQVV9kcHXxodwFbE1ySndc4UJg10ibPwR+FiDJJga7ivZNV7okaSVMEwTfrqqi262T5JnTrLiqngQuB24FPgfcUlV7klyT5Pyu2a3Al5PsBW4D3lpVX15qJyRJh2+as4Y+0p01dGySS4BLgRumWXlV7QZ2j8y7auh9MbiNxRVTVyxJWlGHDIKq+rUk5wJPAD8B/Puq+njvlUmSZmLS3Uc/UVUvA+g++P3wl6R1aNIxAs/TlKQGTNo1dGySf7zYwqr6SA/1SJJmbGIQMLi/0GIXhhkEkrQOTAqCL1TV62ZWiSRpVUw6RjBuJCBJWmcmBcGrZlaFJGnVLBoEVXX/LAuRJK2OaW4xIUlaxyYGQZINSX5/VsVIkmZvYhBU1XeBzd3dQyVJ69A0N537PPDpJLvobkUNUFW/2VdRkqTZmSYIHuleTwOO7rccSdKsTXP30V8FSHL0YLK+3ntVkqSZmeZRlacnuQe4H9iT5O4kL+y/NEnSLExz+uhO4IqqOrmqTgbeAvxOv2VJkmZlmiB4VlXd9tREVX0SeFZvFUmSZmqag8X7kvw74MZu+mLgwf5KkiTN0jQjgtcxeEjNR7rXJuCSPouSJM3OpEdV3lhVrwJeXVVvmmFNkqQZmjQi+PtJTgZel+Q5SX5w+DWrAiVJ/Zp0jOB64I+AU4G7+f+fT1DdfEnSGjfpNtTvqaoXADdU1alVdcrQyxCQpHXikAeLq+oNsyhEkrQ6fB6BJDXOIJCkxi05CLqH1fyzPoqRJM3eokGQ5Jgkv5zkvyR5WQbeCOwDXjm7EiVJfZp0+uiNwFeAzwD/HHgrcASwo6runUFtkqQZmBQEp1bVjwEk+V3gS8BJVfW1mVQmSZqJSccIvvPUm+7ZxQ8uNQSSbE/yQJKFJFdOaHdBkkoyt5T1S5KWb9KI4CeSfJXvXVH8jKHpqqpjJq04yQbgOuDngP3AXUl2VdXekXZHA28C7jzMPkiSlmHSlcUbquqYqjq6e20cmp4YAp2zgIWq2ldVTwA3ATvGtHsH8C7gW4fVA0nSskw6a+jIJG/uzhq6LMk0zy4Ydjzw8ND0/m7e8DbOBE6sqo9NWlG3/fkk8wcOHFhiGZKkSSYdI3g/MAf8BXAe8BtLXHfGzKu/W5g8DbiWwaMvJ6qqnVU1V1VzmzdvXmIZkqRJJn3L3zZ01tB7gT9f4rr3AycOTZ8APDI0fTRwOvDJJAA/BOxKcn5VzS9xW5KkwzTtWUNPHsa67wK2JjklyRHAhcCuoXU+XlWbqmpLVW0B7gAMAUmasUkjgjO6s4RgsJtnSWcNVdWTSS4HbgU2MLid9Z4k1wDzVbVr0s9LkmZjUhB8tqrOXM7Kq2o3sHtk3lWLtD17OduSJB2eSbuGasIySdI6MWlE8LwkVyy2sKp+s4d6JEkzNikINgBHMf40UEnSOjEpCB6tqmtmVokkaVVMOkbgSECSGjApCM6ZWRWSpFUz6aZzfz3LQiRJq8OH10tS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUuF6DIMn2JA8kWUhy5ZjlVyTZm+S+JH+S5OQ+65EkHay3IEiyAbgOOBfYBlyUZNtIs3uAuar6ceDDwLv6qkeSNF6fI4KzgIWq2ldVTwA3ATuGG1TVbVX1zW7yDuCEHuuRJI3RZxAcDzw8NL2/m7eYS4GPj1uQ5LIk80nmDxw4sIIlSpL6DIKMmVdjGyYXA3PAu8ctr6qdVTVXVXObN29ewRIlSRt7XPd+4MSh6ROAR0YbJXkp8CvAz1TVt3usR5I0Rp8jgruArUlOSXIEcCGwa7hBkjOB3wbOr6rHeqxFkrSI3oKgqp4ELgduBT4H3FJVe5Jck+T8rtm7gaOADyW5N8muRVYnSepJn7uGqKrdwO6ReVcNvX9pn9uXJB2aVxZLUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktS4XoMgyfYkDyRZSHLlmOVPT3Jzt/zOJFv6rEeSdLDegiDJBuA64FxgG3BRkm0jzS4FvlJVzweuBX6tr3okSeP1OSI4C1ioqn1V9QRwE7BjpM0O4P3d+w8D5yRJjzVJkkZs7HHdxwMPD03vB/7BYm2q6skkjwPPBb403CjJZcBlACeddNJhFbPth485rJ+TpPWuzyAY982+DqMNVbUT2AkwNzd30PJpXP2KFx7Oj0nSutfnrqH9wIlD0ycAjyzWJslG4Fjgr3usSZI0os8guAvYmuSUJEcAFwK7RtrsAl7Tvb8A+NOqOqxv/JKkw9PbrqFun//lwK3ABuCGqtqT5Bpgvqp2Ae8FbkyywGAkcGFf9UiSxuvzGAFVtRvYPTLvqqH33wL+SZ81SJIm88piSWqcQSBJjTMIJKlxBoEkNS5r7WzNJAeALxzmj29i5KrlBtjnNtjnNiynzydX1eZxC9ZcECxHkvmqmlvtOmbJPrfBPrehrz67a0iSGmcQSFLjWguCnatdwCqwz22wz23opc9NHSOQJB2stRGBJGmEQSBJjVuXQZBke5IHkiwkuXLM8qcnublbfmeSLbOvcmVN0ecrkuxNcl+SP0ly8mrUuZIO1eehdhckqSRr/lTDafqc5JXd73pPkg/MusaVNsXf9klJbktyT/f3fd5q1LlSktyQ5LEk9y+yPEne0/3/uC/Ji5a90apaVy8Gt7z+K+BU4Ajgs8C2kTb/Cri+e38hcPNq1z2DPv8s8Mzu/Rta6HPX7mjgU8AdwNxq1z2D3/NW4B7gOd3081a77hn0eSfwhu79NuDzq133Mvv808CLgPsXWX4e8HEGT3h8MXDncre5HkcEZwELVbWvqp4AbgJ2jLTZAby/e/9h4Jwk4x6buVYcss9VdVtVfbObvIPBE+PWsml+zwDvAN4FfGuWxfVkmj6/Hriuqr4CUFWPzbjGlTZNnwt46qHkx3LwkxDXlKr6FJOf1LgD+L0auAN4dpLjlrPN9RgExwMPD03v7+aNbVNVTwKPA8+dSXX9mKbPwy5l8I1iLTtkn5OcCZxYVR+bZWE9mub3fBpwWpJPJ7kjyfaZVdePafr8duDiJPsZPP/kjbMpbdUs9d/7IfX6YJpVMu6b/eg5stO0WUum7k+Si4E54Gd6rah/E/uc5GnAtcBrZ1XQDEzze97IYPfQ2QxGfX+W5PSq+r8919aXafp8EfC+qvqNJD/J4KmHp1fV3/Zf3qpY8c+v9Tgi2A+cODR9AgcPFf+uTZKNDIaTk4Zi3++m6TNJXgr8CnB+VX17RrX15VB9Pho4Hfhkks8z2Je6a40fMJ72b/ujVfWdqnoQeIBBMKxV0/T5UuAWgKr6DHAkg5uzrVdT/XtfivUYBHcBW5OckuQIBgeDd4202QW8pnt/AfCn1R2FWaMO2eduN8lvMwiBtb7fGA7R56p6vKo2VdWWqtrC4LjI+VU1vzrlrohp/rb/kMGJASTZxGBX0b6ZVrmypunzQ8A5AElewCAIDsy0ytnaBby6O3voxcDjVfXocla47nYNVdWTSS4HbmVwxsENVbUnyTXAfFXtAt7LYPi4wGAkcOHqVbx8U/b53cBRwIe64+IPVdX5q1b0Mk3Z53Vlyj7fCrwsyV7gu8Bbq+rLq1f18kzZ57cAv5PklxjsInntWv5il+SDDHbtbeqOe1wN/ABAVV3P4DjIecAC8E3gkmVvcw3//5IkrYD1uGtIkrQEBoEkNc4gkKTGGQSS1DiDQJIaZxBo3Ury3ST3Dr22JDk7yePdnSo/l+Tqru3w/P+V5NcnrPeD3V0ff+kwarpkqJ4nkvxF9/6dS1zPDyb5l0vdvjSOp49q3Ury9ao6amTe2cC/qaqXJ3kWcC+D60iOHpr/DAZ38Ly0qj498vM/xOBuj1PfxjvJxu6eVqPzP8/gjqhfWmLXSPJ84MNVdcZSf1Ya5YhAzaqqbwB3Az8yMv9vGATEuBt5fQJ4Xvct/iVJzuhu7nZfkj9I8hyAJJ9M8h+S3A7862nqSXJUkvcl+fNuZPKKbv6PJbmr2+Z9SU4F3gn86OGMJqRR6+7KYmnIM5Lc271/sKr+0fDCJM9lcA+idwCbh+Y/h8H9eT41Zp3nAx976pt4kvuAN1bV7d3VrlcDb+7aPruqlnJzv6uAP6qq13Y13Jnkjxk8P+PXq+rmJE9ncNOxK4HnOyLQSjAItJ79zSIflC9Jcg/wt8A7u1sWnN3Nvw/40W7+/5608iTHMviwv72b9X7gQ0NNbl5ivS8Dzs33nsJ1JHAS8D+Bt2XwVLmPVNXC2n58hr7fGARq0Z9V1csXm5/kNOB/JPmDqrp3TLtpfWOJ7QP8QlX91cj8v0zyGeDngT9O8hrW+MNX9P3FYwTSiKr6S+A/Av/2EO0eB76S5CXdrFcBt0/4kUO5FXjTUxPdHWNJcmpVLVTVfwb+O/DjwNcYHOCWls0gkMa7HvjpJKccot1rgHd3u5TOAK5ZxjZ/FXhmd0rpHgZP3gL4pxk8iP5eBs/u/f2q+j/AfNfWg8VaFk8flaTGOSKQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlx/w9FEr/5tvQd3AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ROC_df_test['FPR'],ROC_df_test['TPR'])\n",
    "plt.xlabel('FPR for Test')\n",
    "plt.ylabel('TPR for Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC for test is : \n",
      " 0.9954780361757106\n"
     ]
    }
   ],
   "source": [
    "AUC_test = metrics.auc(ROC_df_test['FPR'],ROC_df_test['TPR'])\n",
    "print('The AUC for test is : \\n',AUC_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the various curves, matrices, etc. have been shown for a random Monte Carlo iteration (here only 1 such iteration) and thus won't be shown in the final M=30 Monte-Carlo iterations for a random iteration within. These results are for any arbitrary such random Monte-Carlo trial, in general\n",
    "\n",
    "\n",
    "NOTE :\n",
    "After X_train, y_train have been generated via labeled and unlabeled train, the remaining portion of the code is pretty much the same as for the Supervised case due to the face that post-generation of X_train and y_train, we only use X_train, y_train, X_test, y_test for all are subsequent evaluations of various quantities\n",
    "\n",
    "Again, here CalibratedCV was used to estimate probabilities which are unavailable inherently for LinearSVC\n",
    "\n",
    "The references mentioned in Supervised_Test were used here as well\n",
    "\n",
    "Comparison will be done later, after all methods are done with"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
